[2019-11-06 07:54:27.316545 UTC] Starting env pool
[2019-11-06 07:54:27.380138 UTC] Starting iteration 0
[2019-11-06 07:54:27.380477 UTC] Start collecting samples
[2019-11-06 07:54:27.669907 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:27.731265 UTC] Computing policy gradient
[2019-11-06 07:54:27.746205 UTC] Updating baseline
[2019-11-06 07:54:27.882538 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| SurrLoss             | -0.0026496 |
| Entropy              | 0.6925     |
| Perplexity           | 1.9987     |
| AveragePolicyProb[0] | 0.50155    |
| AveragePolicyProb[1] | 0.49845    |
| AverageReturn        | 23.462     |
| MinReturn            | 9          |
| MaxReturn            | 81         |
| StdReturn            | 11.748     |
| AverageEpisodeLength | 23.462     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 81         |
| StdEpisodeLength     | 11.748     |
| TotalNEpisodes       | 78         |
| TotalNSamples        | 1830       |
| ExplainedVariance    | -0.0058665 |
-------------------------------------
[2019-11-06 07:54:27.907231 UTC] Saving snapshot
[2019-11-06 07:54:27.914674 UTC] Starting iteration 1
[2019-11-06 07:54:27.914838 UTC] Start collecting samples
[2019-11-06 07:54:28.145593 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:28.185245 UTC] Computing policy gradient
[2019-11-06 07:54:28.195379 UTC] Updating baseline
[2019-11-06 07:54:28.331324 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| SurrLoss             | -0.028403 |
| Entropy              | 0.63881   |
| Perplexity           | 1.8942    |
| AveragePolicyProb[0] | 0.48601   |
| AveragePolicyProb[1] | 0.51399   |
| AverageReturn        | 30.72     |
| MinReturn            | 9         |
| MaxReturn            | 109       |
| StdReturn            | 18.103    |
| AverageEpisodeLength | 30.72     |
| MinEpisodeLength     | 9         |
| MaxEpisodeLength     | 109       |
| StdEpisodeLength     | 18.103    |
| TotalNEpisodes       | 124       |
| TotalNSamples        | 3619      |
| ExplainedVariance    | 0.15902   |
------------------------------------
[2019-11-06 07:54:28.371610 UTC] Saving snapshot
[2019-11-06 07:54:28.380712 UTC] Starting iteration 2
[2019-11-06 07:54:28.380910 UTC] Start collecting samples
[2019-11-06 07:54:28.659247 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:28.687786 UTC] Computing policy gradient
[2019-11-06 07:54:28.700134 UTC] Updating baseline
[2019-11-06 07:54:28.834150 UTC] Computing logging information
------------------------------------
| Iteration            | 2         |
| SurrLoss             | -0.044707 |
| Entropy              | 0.60104   |
| Perplexity           | 1.824     |
| AveragePolicyProb[0] | 0.48011   |
| AveragePolicyProb[1] | 0.51989   |
| AverageReturn        | 38.42     |
| MinReturn            | 10        |
| MaxReturn            | 112       |
| StdReturn            | 22.32     |
| AverageEpisodeLength | 38.42     |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 112       |
| StdEpisodeLength     | 22.32     |
| TotalNEpisodes       | 148       |
| TotalNSamples        | 5017      |
| ExplainedVariance    | 0.33974   |
------------------------------------
[2019-11-06 07:54:28.875231 UTC] Saving snapshot
[2019-11-06 07:54:28.892228 UTC] Starting iteration 3
[2019-11-06 07:54:28.892415 UTC] Start collecting samples
[2019-11-06 07:54:29.113058 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:29.136847 UTC] Computing policy gradient
[2019-11-06 07:54:29.145995 UTC] Updating baseline
[2019-11-06 07:54:29.258610 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| SurrLoss             | -0.021752 |
| Entropy              | 0.56557   |
| Perplexity           | 1.7605    |
| AveragePolicyProb[0] | 0.51612   |
| AveragePolicyProb[1] | 0.48388   |
| AverageReturn        | 53.1      |
| MinReturn            | 10        |
| MaxReturn            | 200       |
| StdReturn            | 42.011    |
| AverageEpisodeLength | 53.1      |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 42.011    |
| TotalNEpisodes       | 161       |
| TotalNSamples        | 6783      |
| ExplainedVariance    | 0.33004   |
------------------------------------
[2019-11-06 07:54:29.283783 UTC] Saving snapshot
[2019-11-06 07:54:29.293468 UTC] Starting iteration 4
[2019-11-06 07:54:29.293644 UTC] Start collecting samples
[2019-11-06 07:54:29.517152 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:29.542168 UTC] Computing policy gradient
[2019-11-06 07:54:29.552690 UTC] Updating baseline
[2019-11-06 07:54:29.732202 UTC] Computing logging information
-----------------------------------
| Iteration            | 4        |
| SurrLoss             | -0.01343 |
| Entropy              | 0.52271  |
| Perplexity           | 1.6866   |
| AveragePolicyProb[0] | 0.49949  |
| AveragePolicyProb[1] | 0.50051  |
| AverageReturn        | 68.93    |
| MinReturn            | 10       |
| MaxReturn            | 200      |
| StdReturn            | 52.911   |
| AverageEpisodeLength | 68.93    |
| MinEpisodeLength     | 10       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 52.911   |
| TotalNEpisodes       | 173      |
| TotalNSamples        | 8606     |
| ExplainedVariance    | 0.76978  |
-----------------------------------
[2019-11-06 07:54:29.770230 UTC] Saving snapshot
[2019-11-06 07:54:29.786200 UTC] Starting iteration 5
[2019-11-06 07:54:29.786559 UTC] Start collecting samples
[2019-11-06 07:54:30.111676 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:30.139177 UTC] Computing policy gradient
[2019-11-06 07:54:30.151181 UTC] Updating baseline
[2019-11-06 07:54:30.313401 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| SurrLoss             | -0.011786 |
| Entropy              | 0.48944   |
| Perplexity           | 1.6314    |
| AveragePolicyProb[0] | 0.50122   |
| AveragePolicyProb[1] | 0.49878   |
| AverageReturn        | 84.48     |
| MinReturn            | 16        |
| MaxReturn            | 200       |
| StdReturn            | 59.894    |
| AverageEpisodeLength | 84.48     |
| MinEpisodeLength     | 16        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 59.894    |
| TotalNEpisodes       | 183       |
| TotalNSamples        | 10391     |
| ExplainedVariance    | 0.72004   |
------------------------------------
[2019-11-06 07:54:30.359142 UTC] Saving snapshot
[2019-11-06 07:54:30.374577 UTC] Starting iteration 6
[2019-11-06 07:54:30.375390 UTC] Start collecting samples
[2019-11-06 07:54:31.236054 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:31.266673 UTC] Computing policy gradient
[2019-11-06 07:54:31.278292 UTC] Updating baseline
[2019-11-06 07:54:31.433629 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| SurrLoss             | -0.023091 |
| Entropy              | 0.45278   |
| Perplexity           | 1.5727    |
| AveragePolicyProb[0] | 0.492     |
| AveragePolicyProb[1] | 0.508     |
| AverageReturn        | 102.91    |
| MinReturn            | 18        |
| MaxReturn            | 200       |
| StdReturn            | 62.442    |
| AverageEpisodeLength | 102.91    |
| MinEpisodeLength     | 18        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 62.442    |
| TotalNEpisodes       | 197       |
| TotalNSamples        | 12648     |
| ExplainedVariance    | 0.67661   |
------------------------------------
[2019-11-06 07:54:31.466702 UTC] Saving snapshot
[2019-11-06 07:54:31.476193 UTC] Starting iteration 7
[2019-11-06 07:54:31.476414 UTC] Start collecting samples
[2019-11-06 07:54:31.979178 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:32.025363 UTC] Computing policy gradient
[2019-11-06 07:54:32.042179 UTC] Updating baseline
[2019-11-06 07:54:32.254525 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| SurrLoss             | -0.016464 |
| Entropy              | 0.42004   |
| Perplexity           | 1.522     |
| AveragePolicyProb[0] | 0.50509   |
| AveragePolicyProb[1] | 0.49491   |
| AverageReturn        | 119.87    |
| MinReturn            | 18        |
| MaxReturn            | 200       |
| StdReturn            | 61.119    |
| AverageEpisodeLength | 119.87    |
| MinEpisodeLength     | 18        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 61.119    |
| TotalNEpisodes       | 211       |
| TotalNSamples        | 14932     |
| ExplainedVariance    | 0.67818   |
------------------------------------
[2019-11-06 07:54:32.300614 UTC] Saving snapshot
[2019-11-06 07:54:32.314657 UTC] Starting iteration 8
[2019-11-06 07:54:32.314957 UTC] Start collecting samples
[2019-11-06 07:54:32.684205 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:32.721027 UTC] Computing policy gradient
[2019-11-06 07:54:32.737223 UTC] Updating baseline
[2019-11-06 07:54:32.977072 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| SurrLoss             | 0.0022306 |
| Entropy              | 0.3878    |
| Perplexity           | 1.4737    |
| AveragePolicyProb[0] | 0.50556   |
| AveragePolicyProb[1] | 0.49444   |
| AverageReturn        | 128.76    |
| MinReturn            | 29        |
| MaxReturn            | 200       |
| StdReturn            | 58.596    |
| AverageEpisodeLength | 128.76    |
| MinEpisodeLength     | 29        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 58.596    |
| TotalNEpisodes       | 219       |
| TotalNSamples        | 16195     |
| ExplainedVariance    | 0.76633   |
------------------------------------
[2019-11-06 07:54:33.021889 UTC] Saving snapshot
[2019-11-06 07:54:33.035264 UTC] Starting iteration 9
[2019-11-06 07:54:33.035591 UTC] Start collecting samples
[2019-11-06 07:54:33.626752 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:33.661686 UTC] Computing policy gradient
[2019-11-06 07:54:33.676886 UTC] Updating baseline
[2019-11-06 07:54:33.867477 UTC] Computing logging information
-------------------------------------
| Iteration            | 9          |
| SurrLoss             | -0.0028893 |
| Entropy              | 0.36246    |
| Perplexity           | 1.4369     |
| AveragePolicyProb[0] | 0.5021     |
| AveragePolicyProb[1] | 0.4979     |
| AverageReturn        | 142.68     |
| MinReturn            | 29         |
| MaxReturn            | 200        |
| StdReturn            | 54.707     |
| AverageEpisodeLength | 142.68     |
| MinEpisodeLength     | 29         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 54.707     |
| TotalNEpisodes       | 230        |
| TotalNSamples        | 18204      |
| ExplainedVariance    | 0.82493    |
-------------------------------------
[2019-11-06 07:54:33.909946 UTC] Saving snapshot
[2019-11-06 07:54:33.919489 UTC] Starting iteration 10
[2019-11-06 07:54:33.920210 UTC] Start collecting samples
[2019-11-06 07:54:34.274621 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:34.307133 UTC] Computing policy gradient
[2019-11-06 07:54:34.317554 UTC] Updating baseline
[2019-11-06 07:54:34.508467 UTC] Computing logging information
-----------------------------------
| Iteration            | 10       |
| SurrLoss             | 0.014146 |
| Entropy              | 0.33789  |
| Perplexity           | 1.402    |
| AveragePolicyProb[0] | 0.51755  |
| AveragePolicyProb[1] | 0.48245  |
| AverageReturn        | 161.44   |
| MinReturn            | 33       |
| MaxReturn            | 200      |
| StdReturn            | 45.801   |
| AverageEpisodeLength | 161.44   |
| MinEpisodeLength     | 33       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 45.801   |
| TotalNEpisodes       | 244      |
| TotalNSamples        | 20963    |
| ExplainedVariance    | 0.68108  |
-----------------------------------
[2019-11-06 07:54:34.541920 UTC] Saving snapshot
[2019-11-06 07:54:34.551272 UTC] Starting iteration 11
[2019-11-06 07:54:34.551484 UTC] Start collecting samples
[2019-11-06 07:54:34.949253 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:34.981157 UTC] Computing policy gradient
[2019-11-06 07:54:34.997024 UTC] Updating baseline
[2019-11-06 07:54:35.167257 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| SurrLoss             | -0.010883 |
| Entropy              | 0.32872   |
| Perplexity           | 1.3892    |
| AveragePolicyProb[0] | 0.50891   |
| AveragePolicyProb[1] | 0.49109   |
| AverageReturn        | 168.99    |
| MinReturn            | 64        |
| MaxReturn            | 200       |
| StdReturn            | 38.386    |
| AverageEpisodeLength | 168.99    |
| MinEpisodeLength     | 64        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 38.386    |
| TotalNEpisodes       | 250       |
| TotalNSamples        | 22163     |
| ExplainedVariance    | 0.91311   |
------------------------------------
[2019-11-06 07:54:35.215619 UTC] Saving snapshot
[2019-11-06 07:54:35.227950 UTC] Starting iteration 12
[2019-11-06 07:54:35.228841 UTC] Start collecting samples
[2019-11-06 07:54:35.615353 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:35.648473 UTC] Computing policy gradient
[2019-11-06 07:54:35.661806 UTC] Updating baseline
[2019-11-06 07:54:35.856168 UTC] Computing logging information
-----------------------------------
| Iteration            | 12       |
| SurrLoss             | 0.020091 |
| Entropy              | 0.30299  |
| Perplexity           | 1.3539   |
| AveragePolicyProb[0] | 0.50256  |
| AveragePolicyProb[1] | 0.49744  |
| AverageReturn        | 175.57   |
| MinReturn            | 64       |
| MaxReturn            | 200      |
| StdReturn            | 35.272   |
| AverageEpisodeLength | 175.57   |
| MinEpisodeLength     | 64       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 35.272   |
| TotalNEpisodes       | 260      |
| TotalNSamples        | 24140    |
| ExplainedVariance    | 0.56413  |
-----------------------------------
[2019-11-06 07:54:35.890169 UTC] Saving snapshot
[2019-11-06 07:54:35.899062 UTC] Starting iteration 13
[2019-11-06 07:54:35.899265 UTC] Start collecting samples
[2019-11-06 07:54:36.323052 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:36.364710 UTC] Computing policy gradient
[2019-11-06 07:54:36.379366 UTC] Updating baseline
[2019-11-06 07:54:36.564500 UTC] Computing logging information
-----------------------------------
| Iteration            | 13       |
| SurrLoss             | 0.013758 |
| Entropy              | 0.29758  |
| Perplexity           | 1.3466   |
| AveragePolicyProb[0] | 0.51499  |
| AveragePolicyProb[1] | 0.48501  |
| AverageReturn        | 179.78   |
| MinReturn            | 80       |
| MaxReturn            | 200      |
| StdReturn            | 31.141   |
| AverageEpisodeLength | 179.78   |
| MinEpisodeLength     | 80       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 31.141   |
| TotalNEpisodes       | 273      |
| TotalNSamples        | 26584    |
| ExplainedVariance    | 0.77148  |
-----------------------------------
[2019-11-06 07:54:36.607561 UTC] Saving snapshot
[2019-11-06 07:54:36.620226 UTC] Starting iteration 14
[2019-11-06 07:54:36.620724 UTC] Start collecting samples
[2019-11-06 07:54:37.041982 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:37.084609 UTC] Computing policy gradient
[2019-11-06 07:54:37.101247 UTC] Updating baseline
[2019-11-06 07:54:37.287604 UTC] Computing logging information
------------------------------------
| Iteration            | 14        |
| SurrLoss             | 0.0056843 |
| Entropy              | 0.29567   |
| Perplexity           | 1.344     |
| AveragePolicyProb[0] | 0.53547   |
| AveragePolicyProb[1] | 0.46453   |
| AverageReturn        | 180.73    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 30.907    |
| AverageEpisodeLength | 180.73    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 30.907    |
| TotalNEpisodes       | 284       |
| TotalNSamples        | 28630     |
| ExplainedVariance    | 0.81334   |
------------------------------------
[2019-11-06 07:54:37.343316 UTC] Saving snapshot
[2019-11-06 07:54:37.358148 UTC] Starting iteration 15
[2019-11-06 07:54:37.358887 UTC] Start collecting samples
[2019-11-06 07:54:37.804445 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:37.840942 UTC] Computing policy gradient
[2019-11-06 07:54:37.854814 UTC] Updating baseline
[2019-11-06 07:54:38.043861 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| SurrLoss             | 0.0039076 |
| Entropy              | 0.29026   |
| Perplexity           | 1.3368    |
| AveragePolicyProb[0] | 0.53314   |
| AveragePolicyProb[1] | 0.46686   |
| AverageReturn        | 182.4     |
| MinReturn            | 84        |
| MaxReturn            | 200       |
| StdReturn            | 26.478    |
| AverageEpisodeLength | 182.4     |
| MinEpisodeLength     | 84        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 26.478    |
| TotalNEpisodes       | 296       |
| TotalNSamples        | 30720     |
| ExplainedVariance    | 0.89693   |
------------------------------------
[2019-11-06 07:54:38.097826 UTC] Saving snapshot
[2019-11-06 07:54:38.115459 UTC] Starting iteration 16
[2019-11-06 07:54:38.115778 UTC] Start collecting samples
[2019-11-06 07:54:38.554670 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:38.582345 UTC] Computing policy gradient
[2019-11-06 07:54:38.596369 UTC] Updating baseline
[2019-11-06 07:54:38.752560 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| SurrLoss             | -0.018331 |
| Entropy              | 0.29744   |
| Perplexity           | 1.3464    |
| AveragePolicyProb[0] | 0.51573   |
| AveragePolicyProb[1] | 0.48427   |
| AverageReturn        | 185.31    |
| MinReturn            | 84        |
| MaxReturn            | 200       |
| StdReturn            | 22.824    |
| AverageEpisodeLength | 185.31    |
| MinEpisodeLength     | 84        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 22.824    |
| TotalNEpisodes       | 307       |
| TotalNSamples        | 32743     |
| ExplainedVariance    | 0.88666   |
------------------------------------
[2019-11-06 07:54:38.788225 UTC] Saving snapshot
[2019-11-06 07:54:38.798873 UTC] Starting iteration 17
[2019-11-06 07:54:38.799094 UTC] Start collecting samples
[2019-11-06 07:54:39.161554 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:39.196239 UTC] Computing policy gradient
[2019-11-06 07:54:39.209745 UTC] Updating baseline
[2019-11-06 07:54:39.413317 UTC] Computing logging information
------------------------------------
| Iteration            | 17        |
| SurrLoss             | -0.023674 |
| Entropy              | 0.29992   |
| Perplexity           | 1.3498    |
| AveragePolicyProb[0] | 0.49612   |
| AveragePolicyProb[1] | 0.50388   |
| AverageReturn        | 188.4     |
| MinReturn            | 106       |
| MaxReturn            | 200       |
| StdReturn            | 19.282    |
| AverageEpisodeLength | 188.4     |
| MinEpisodeLength     | 106       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 19.282    |
| TotalNEpisodes       | 315       |
| TotalNSamples        | 34343     |
| ExplainedVariance    | 0.73718   |
------------------------------------
[2019-11-06 07:54:39.450038 UTC] Saving snapshot
[2019-11-06 07:54:39.460887 UTC] Starting iteration 18
[2019-11-06 07:54:39.461091 UTC] Start collecting samples
[2019-11-06 07:54:39.816349 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:39.846123 UTC] Computing policy gradient
[2019-11-06 07:54:39.860443 UTC] Updating baseline
[2019-11-06 07:54:40.056098 UTC] Computing logging information
------------------------------------
| Iteration            | 18        |
| SurrLoss             | 0.0011825 |
| Entropy              | 0.28939   |
| Perplexity           | 1.3356    |
| AveragePolicyProb[0] | 0.49316   |
| AveragePolicyProb[1] | 0.50684   |
| AverageReturn        | 190.43    |
| MinReturn            | 131       |
| MaxReturn            | 200       |
| StdReturn            | 17.027    |
| AverageEpisodeLength | 190.43    |
| MinEpisodeLength     | 131       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 17.027    |
| TotalNEpisodes       | 324       |
| TotalNSamples        | 36143     |
| ExplainedVariance    | 0.57379   |
------------------------------------
[2019-11-06 07:54:40.095438 UTC] Saving snapshot
[2019-11-06 07:54:40.107697 UTC] Starting iteration 19
[2019-11-06 07:54:40.107921 UTC] Start collecting samples
[2019-11-06 07:54:40.523718 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:40.564244 UTC] Computing policy gradient
[2019-11-06 07:54:40.577971 UTC] Updating baseline
[2019-11-06 07:54:40.819894 UTC] Computing logging information
-------------------------------------
| Iteration            | 19         |
| SurrLoss             | -0.0085643 |
| Entropy              | 0.28966    |
| Perplexity           | 1.336      |
| AveragePolicyProb[0] | 0.50282    |
| AveragePolicyProb[1] | 0.49718    |
| AverageReturn        | 191.76     |
| MinReturn            | 144        |
| MaxReturn            | 200        |
| StdReturn            | 15.87      |
| AverageEpisodeLength | 191.76     |
| MinEpisodeLength     | 144        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.87      |
| TotalNEpisodes       | 336        |
| TotalNSamples        | 38543      |
| ExplainedVariance    | 0.355      |
-------------------------------------
[2019-11-06 07:54:40.864532 UTC] Saving snapshot
[2019-11-06 07:54:40.876537 UTC] Starting iteration 20
[2019-11-06 07:54:40.877319 UTC] Start collecting samples
[2019-11-06 07:54:41.277527 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:41.313030 UTC] Computing policy gradient
[2019-11-06 07:54:41.327645 UTC] Updating baseline
[2019-11-06 07:54:41.540060 UTC] Computing logging information
--------------------------------------
| Iteration            | 20          |
| SurrLoss             | -0.00011771 |
| Entropy              | 0.27784     |
| Perplexity           | 1.3203      |
| AveragePolicyProb[0] | 0.49682     |
| AveragePolicyProb[1] | 0.50318     |
| AverageReturn        | 191.8       |
| MinReturn            | 144         |
| MaxReturn            | 200         |
| StdReturn            | 15.886      |
| AverageEpisodeLength | 191.8       |
| MinEpisodeLength     | 144         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 15.886      |
| TotalNEpisodes       | 344         |
| TotalNSamples        | 40143       |
| ExplainedVariance    | 0.58615     |
--------------------------------------
[2019-11-06 07:54:41.612277 UTC] Saving snapshot
[2019-11-06 07:54:41.622258 UTC] Starting iteration 21
[2019-11-06 07:54:41.622438 UTC] Start collecting samples
[2019-11-06 07:54:42.023610 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:42.056388 UTC] Computing policy gradient
[2019-11-06 07:54:42.068765 UTC] Updating baseline
[2019-11-06 07:54:42.271124 UTC] Computing logging information
-------------------------------------
| Iteration            | 21         |
| SurrLoss             | -0.0073582 |
| Entropy              | 0.2709     |
| Perplexity           | 1.3111     |
| AveragePolicyProb[0] | 0.5097     |
| AveragePolicyProb[1] | 0.49031    |
| AverageReturn        | 191.95     |
| MinReturn            | 144        |
| MaxReturn            | 200        |
| StdReturn            | 15.892     |
| AverageEpisodeLength | 191.95     |
| MinEpisodeLength     | 144        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.892     |
| TotalNEpisodes       | 356        |
| TotalNSamples        | 42543      |
| ExplainedVariance    | 0.11105    |
-------------------------------------
[2019-11-06 07:54:42.318301 UTC] Saving snapshot
[2019-11-06 07:54:42.330110 UTC] Starting iteration 22
[2019-11-06 07:54:42.330394 UTC] Start collecting samples
[2019-11-06 07:54:42.682483 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:42.713021 UTC] Computing policy gradient
[2019-11-06 07:54:42.726669 UTC] Updating baseline
[2019-11-06 07:54:42.894634 UTC] Computing logging information
-------------------------------------
| Iteration            | 22         |
| SurrLoss             | 6.8436e-06 |
| Entropy              | 0.2683     |
| Perplexity           | 1.3077     |
| AveragePolicyProb[0] | 0.50357    |
| AveragePolicyProb[1] | 0.49643    |
| AverageReturn        | 192.03     |
| MinReturn            | 144        |
| MaxReturn            | 200        |
| StdReturn            | 15.912     |
| AverageEpisodeLength | 192.03     |
| MinEpisodeLength     | 144        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.912     |
| TotalNEpisodes       | 365        |
| TotalNSamples        | 44343      |
| ExplainedVariance    | 0.45192    |
-------------------------------------
[2019-11-06 07:54:42.939792 UTC] Saving snapshot
[2019-11-06 07:54:42.951867 UTC] Starting iteration 23
[2019-11-06 07:54:42.952602 UTC] Start collecting samples
[2019-11-06 07:54:43.325710 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:43.356780 UTC] Computing policy gradient
[2019-11-06 07:54:43.372302 UTC] Updating baseline
[2019-11-06 07:54:43.538367 UTC] Computing logging information
------------------------------------
| Iteration            | 23        |
| SurrLoss             | 0.0052665 |
| Entropy              | 0.27096   |
| Perplexity           | 1.3112    |
| AveragePolicyProb[0] | 0.51269   |
| AveragePolicyProb[1] | 0.48731   |
| AverageReturn        | 193.59    |
| MinReturn            | 144       |
| MaxReturn            | 200       |
| StdReturn            | 14.559    |
| AverageEpisodeLength | 193.59    |
| MinEpisodeLength     | 144       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 14.559    |
| TotalNEpisodes       | 376       |
| TotalNSamples        | 46543     |
| ExplainedVariance    | 0.27349   |
------------------------------------
[2019-11-06 07:54:43.579570 UTC] Saving snapshot
[2019-11-06 07:54:43.591059 UTC] Starting iteration 24
[2019-11-06 07:54:43.591332 UTC] Start collecting samples
[2019-11-06 07:54:44.005191 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:44.040962 UTC] Computing policy gradient
[2019-11-06 07:54:44.056530 UTC] Updating baseline
[2019-11-06 07:54:44.216103 UTC] Computing logging information
------------------------------------
| Iteration            | 24        |
| SurrLoss             | 0.0009762 |
| Entropy              | 0.259     |
| Perplexity           | 1.2956    |
| AveragePolicyProb[0] | 0.50239   |
| AveragePolicyProb[1] | 0.49761   |
| AverageReturn        | 195.97    |
| MinReturn            | 144       |
| MaxReturn            | 200       |
| StdReturn            | 11.851    |
| AverageEpisodeLength | 195.97    |
| MinEpisodeLength     | 144       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 11.851    |
| TotalNEpisodes       | 387       |
| TotalNSamples        | 48743     |
| ExplainedVariance    | 0.27557   |
------------------------------------
[2019-11-06 07:54:44.255711 UTC] Saving snapshot
[2019-11-06 07:54:44.265642 UTC] Starting iteration 25
[2019-11-06 07:54:44.265864 UTC] Start collecting samples
[2019-11-06 07:54:44.616190 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:44.644128 UTC] Computing policy gradient
[2019-11-06 07:54:44.657085 UTC] Updating baseline
[2019-11-06 07:54:44.866397 UTC] Computing logging information
------------------------------------
| Iteration            | 25        |
| SurrLoss             | -0.032023 |
| Entropy              | 0.26588   |
| Perplexity           | 1.3046    |
| AveragePolicyProb[0] | 0.49839   |
| AveragePolicyProb[1] | 0.50161   |
| AverageReturn        | 197.67    |
| MinReturn            | 144       |
| MaxReturn            | 200       |
| StdReturn            | 9.0896    |
| AverageEpisodeLength | 197.67    |
| MinEpisodeLength     | 144       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.0896    |
| TotalNEpisodes       | 395       |
| TotalNSamples        | 50343     |
| ExplainedVariance    | 0.40014   |
------------------------------------
[2019-11-06 07:54:44.903758 UTC] Saving snapshot
[2019-11-06 07:54:44.914456 UTC] Starting iteration 26
[2019-11-06 07:54:44.914670 UTC] Start collecting samples
[2019-11-06 07:54:45.233623 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:45.259947 UTC] Computing policy gradient
[2019-11-06 07:54:45.273694 UTC] Updating baseline
[2019-11-06 07:54:45.465053 UTC] Computing logging information
------------------------------------
| Iteration            | 26        |
| SurrLoss             | -0.022943 |
| Entropy              | 0.25296   |
| Perplexity           | 1.2878    |
| AveragePolicyProb[0] | 0.49816   |
| AveragePolicyProb[1] | 0.50184   |
| AverageReturn        | 199.88    |
| MinReturn            | 188       |
| MaxReturn            | 200       |
| StdReturn            | 1.194     |
| AverageEpisodeLength | 199.88    |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 1.194     |
| TotalNEpisodes       | 404       |
| TotalNSamples        | 52143     |
| ExplainedVariance    | 0.076352  |
------------------------------------
[2019-11-06 07:54:45.504990 UTC] Saving snapshot
[2019-11-06 07:54:45.516377 UTC] Starting iteration 27
[2019-11-06 07:54:45.516586 UTC] Start collecting samples
[2019-11-06 07:54:45.821795 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:45.850628 UTC] Computing policy gradient
[2019-11-06 07:54:45.864584 UTC] Updating baseline
[2019-11-06 07:54:46.052865 UTC] Computing logging information
-------------------------------------
| Iteration            | 27         |
| SurrLoss             | -0.0095373 |
| Entropy              | 0.24784    |
| Perplexity           | 1.2813     |
| AveragePolicyProb[0] | 0.51072    |
| AveragePolicyProb[1] | 0.48928    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 416        |
| TotalNSamples        | 54543      |
| ExplainedVariance    | 0.27292    |
-------------------------------------
[2019-11-06 07:54:46.092378 UTC] Saving snapshot
[2019-11-06 07:54:46.104192 UTC] Starting iteration 28
[2019-11-06 07:54:46.104406 UTC] Start collecting samples
[2019-11-06 07:54:46.501690 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:46.533199 UTC] Computing policy gradient
[2019-11-06 07:54:46.548707 UTC] Updating baseline
[2019-11-06 07:54:46.753196 UTC] Computing logging information
------------------------------------
| Iteration            | 28        |
| SurrLoss             | -0.015847 |
| Entropy              | 0.24704   |
| Perplexity           | 1.2802    |
| AveragePolicyProb[0] | 0.49776   |
| AveragePolicyProb[1] | 0.50224   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 424       |
| TotalNSamples        | 56143     |
| ExplainedVariance    | 0.47993   |
------------------------------------
[2019-11-06 07:54:46.802661 UTC] Saving snapshot
[2019-11-06 07:54:46.815054 UTC] Starting iteration 29
[2019-11-06 07:54:46.815683 UTC] Start collecting samples
[2019-11-06 07:54:47.282825 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:47.319600 UTC] Computing policy gradient
[2019-11-06 07:54:47.333779 UTC] Updating baseline
[2019-11-06 07:54:47.572934 UTC] Computing logging information
------------------------------------
| Iteration            | 29        |
| SurrLoss             | 0.0075635 |
| Entropy              | 0.24616   |
| Perplexity           | 1.2791    |
| AveragePolicyProb[0] | 0.50721   |
| AveragePolicyProb[1] | 0.49279   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 436       |
| TotalNSamples        | 58543     |
| ExplainedVariance    | 0.23601   |
------------------------------------
[2019-11-06 07:54:47.623857 UTC] Saving snapshot
[2019-11-06 07:54:47.637301 UTC] Starting iteration 30
[2019-11-06 07:54:47.638046 UTC] Start collecting samples
[2019-11-06 07:54:48.071251 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:48.100492 UTC] Computing policy gradient
[2019-11-06 07:54:48.114969 UTC] Updating baseline
[2019-11-06 07:54:48.281601 UTC] Computing logging information
-----------------------------------
| Iteration            | 30       |
| SurrLoss             | 0.005787 |
| Entropy              | 0.22823  |
| Perplexity           | 1.2564   |
| AveragePolicyProb[0] | 0.49127  |
| AveragePolicyProb[1] | 0.50873  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 445      |
| TotalNSamples        | 60343    |
| ExplainedVariance    | 0.29669  |
-----------------------------------
[2019-11-06 07:54:48.323232 UTC] Saving snapshot
[2019-11-06 07:54:48.334549 UTC] Starting iteration 31
[2019-11-06 07:54:48.334761 UTC] Start collecting samples
[2019-11-06 07:54:48.661754 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:48.698045 UTC] Computing policy gradient
[2019-11-06 07:54:48.713915 UTC] Updating baseline
[2019-11-06 07:54:48.922180 UTC] Computing logging information
-------------------------------------
| Iteration            | 31         |
| SurrLoss             | -0.0040467 |
| Entropy              | 0.23133    |
| Perplexity           | 1.2603     |
| AveragePolicyProb[0] | 0.51008    |
| AveragePolicyProb[1] | 0.48992    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 456        |
| TotalNSamples        | 62543      |
| ExplainedVariance    | 0.28968    |
-------------------------------------
[2019-11-06 07:54:48.972105 UTC] Saving snapshot
[2019-11-06 07:54:48.985674 UTC] Starting iteration 32
[2019-11-06 07:54:48.986350 UTC] Start collecting samples
[2019-11-06 07:54:49.324879 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:49.355837 UTC] Computing policy gradient
[2019-11-06 07:54:49.369881 UTC] Updating baseline
[2019-11-06 07:54:49.549627 UTC] Computing logging information
------------------------------------
| Iteration            | 32        |
| SurrLoss             | 0.0025263 |
| Entropy              | 0.22913   |
| Perplexity           | 1.2575    |
| AveragePolicyProb[0] | 0.50072   |
| AveragePolicyProb[1] | 0.49928   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 467       |
| TotalNSamples        | 64743     |
| ExplainedVariance    | 0.33729   |
------------------------------------
[2019-11-06 07:54:49.597531 UTC] Saving snapshot
[2019-11-06 07:54:49.611124 UTC] Starting iteration 33
[2019-11-06 07:54:49.611367 UTC] Start collecting samples
[2019-11-06 07:54:49.958746 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:49.988524 UTC] Computing policy gradient
[2019-11-06 07:54:50.004159 UTC] Updating baseline
[2019-11-06 07:54:50.201261 UTC] Computing logging information
-------------------------------------
| Iteration            | 33         |
| SurrLoss             | -0.0079408 |
| Entropy              | 0.22302    |
| Perplexity           | 1.2498     |
| AveragePolicyProb[0] | 0.49831    |
| AveragePolicyProb[1] | 0.50169    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 475        |
| TotalNSamples        | 66343      |
| ExplainedVariance    | 0.58555    |
-------------------------------------
[2019-11-06 07:54:50.246693 UTC] Saving snapshot
[2019-11-06 07:54:50.258402 UTC] Starting iteration 34
[2019-11-06 07:54:50.258946 UTC] Start collecting samples
[2019-11-06 07:54:50.597676 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:50.626317 UTC] Computing policy gradient
[2019-11-06 07:54:50.638053 UTC] Updating baseline
[2019-11-06 07:54:50.864608 UTC] Computing logging information
------------------------------------
| Iteration            | 34        |
| SurrLoss             | 0.0071279 |
| Entropy              | 0.22612   |
| Perplexity           | 1.2537    |
| AveragePolicyProb[0] | 0.4943    |
| AveragePolicyProb[1] | 0.5057    |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 484       |
| TotalNSamples        | 68143     |
| ExplainedVariance    | 0.44132   |
------------------------------------
[2019-11-06 07:54:50.930581 UTC] Saving snapshot
[2019-11-06 07:54:50.948159 UTC] Starting iteration 35
[2019-11-06 07:54:50.948776 UTC] Start collecting samples
[2019-11-06 07:54:51.380874 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:51.415919 UTC] Computing policy gradient
[2019-11-06 07:54:51.430972 UTC] Updating baseline
[2019-11-06 07:54:51.627832 UTC] Computing logging information
-----------------------------------
| Iteration            | 35       |
| SurrLoss             | 0.002427 |
| Entropy              | 0.20857  |
| Perplexity           | 1.2319   |
| AveragePolicyProb[0] | 0.49903  |
| AveragePolicyProb[1] | 0.50097  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 496      |
| TotalNSamples        | 70543    |
| ExplainedVariance    | 0.38423  |
-----------------------------------
[2019-11-06 07:54:51.670083 UTC] Saving snapshot
[2019-11-06 07:54:51.682641 UTC] Starting iteration 36
[2019-11-06 07:54:51.682894 UTC] Start collecting samples
[2019-11-06 07:54:52.002320 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:52.029606 UTC] Computing policy gradient
[2019-11-06 07:54:52.040922 UTC] Updating baseline
[2019-11-06 07:54:52.230216 UTC] Computing logging information
-----------------------------------
| Iteration            | 36       |
| SurrLoss             | 0.002791 |
| Entropy              | 0.2229   |
| Perplexity           | 1.2497   |
| AveragePolicyProb[0] | 0.49724  |
| AveragePolicyProb[1] | 0.50276  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 504      |
| TotalNSamples        | 72143    |
| ExplainedVariance    | 0.49591  |
-----------------------------------
[2019-11-06 07:54:52.277344 UTC] Saving snapshot
[2019-11-06 07:54:52.288348 UTC] Starting iteration 37
[2019-11-06 07:54:52.290858 UTC] Start collecting samples
[2019-11-06 07:54:52.762182 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:52.797615 UTC] Computing policy gradient
[2019-11-06 07:54:52.812721 UTC] Updating baseline
[2019-11-06 07:54:53.017156 UTC] Computing logging information
-----------------------------------
| Iteration            | 37       |
| SurrLoss             | 0.008625 |
| Entropy              | 0.20529  |
| Perplexity           | 1.2279   |
| AveragePolicyProb[0] | 0.4921   |
| AveragePolicyProb[1] | 0.5079   |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 516      |
| TotalNSamples        | 74543    |
| ExplainedVariance    | 0.25107  |
-----------------------------------
[2019-11-06 07:54:53.065920 UTC] Saving snapshot
[2019-11-06 07:54:53.079665 UTC] Starting iteration 38
[2019-11-06 07:54:53.079899 UTC] Start collecting samples
[2019-11-06 07:54:53.393991 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:53.422139 UTC] Computing policy gradient
[2019-11-06 07:54:53.435592 UTC] Updating baseline
[2019-11-06 07:54:53.652723 UTC] Computing logging information
------------------------------------
| Iteration            | 38        |
| SurrLoss             | -0.010286 |
| Entropy              | 0.21922   |
| Perplexity           | 1.2451    |
| AveragePolicyProb[0] | 0.50004   |
| AveragePolicyProb[1] | 0.49996   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 525       |
| TotalNSamples        | 76343     |
| ExplainedVariance    | 0.44444   |
------------------------------------
[2019-11-06 07:54:53.693349 UTC] Saving snapshot
[2019-11-06 07:54:53.703785 UTC] Starting iteration 39
[2019-11-06 07:54:53.704041 UTC] Start collecting samples
[2019-11-06 07:54:54.070483 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:54.106508 UTC] Computing policy gradient
[2019-11-06 07:54:54.121532 UTC] Updating baseline
[2019-11-06 07:54:54.297816 UTC] Computing logging information
-----------------------------------
| Iteration            | 39       |
| SurrLoss             | 0.013774 |
| Entropy              | 0.21234  |
| Perplexity           | 1.2366   |
| AveragePolicyProb[0] | 0.50515  |
| AveragePolicyProb[1] | 0.49485  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 536      |
| TotalNSamples        | 78543    |
| ExplainedVariance    | 0.32796  |
-----------------------------------
[2019-11-06 07:54:54.340467 UTC] Saving snapshot
[2019-11-06 07:54:54.350816 UTC] Starting iteration 40
[2019-11-06 07:54:54.351043 UTC] Start collecting samples
[2019-11-06 07:54:54.766424 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:54.798211 UTC] Computing policy gradient
[2019-11-06 07:54:54.811583 UTC] Updating baseline
[2019-11-06 07:54:54.981628 UTC] Computing logging information
-------------------------------------
| Iteration            | 40         |
| SurrLoss             | -0.0053414 |
| Entropy              | 0.21783    |
| Perplexity           | 1.2434     |
| AveragePolicyProb[0] | 0.50383    |
| AveragePolicyProb[1] | 0.49617    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 547        |
| TotalNSamples        | 80743      |
| ExplainedVariance    | 0.56323    |
-------------------------------------
[2019-11-06 07:54:55.024516 UTC] Saving snapshot
[2019-11-06 07:54:55.034477 UTC] Starting iteration 41
[2019-11-06 07:54:55.034715 UTC] Start collecting samples
[2019-11-06 07:54:55.463055 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:55.492707 UTC] Computing policy gradient
[2019-11-06 07:54:55.509852 UTC] Updating baseline
[2019-11-06 07:54:55.680357 UTC] Computing logging information
-----------------------------------
| Iteration            | 41       |
| SurrLoss             | 0.013858 |
| Entropy              | 0.23469  |
| Perplexity           | 1.2645   |
| AveragePolicyProb[0] | 0.4953   |
| AveragePolicyProb[1] | 0.5047   |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 555      |
| TotalNSamples        | 82343    |
| ExplainedVariance    | 0.53998  |
-----------------------------------
[2019-11-06 07:54:55.730308 UTC] Saving snapshot
[2019-11-06 07:54:55.742637 UTC] Starting iteration 42
[2019-11-06 07:54:55.742877 UTC] Start collecting samples
[2019-11-06 07:54:56.135179 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:56.165837 UTC] Computing policy gradient
[2019-11-06 07:54:56.180085 UTC] Updating baseline
[2019-11-06 07:54:56.398612 UTC] Computing logging information
-----------------------------------
| Iteration            | 42       |
| SurrLoss             | 0.004802 |
| Entropy              | 0.22141  |
| Perplexity           | 1.2478   |
| AveragePolicyProb[0] | 0.49836  |
| AveragePolicyProb[1] | 0.50164  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 564      |
| TotalNSamples        | 84143    |
| ExplainedVariance    | 0.49174  |
-----------------------------------
[2019-11-06 07:54:56.446530 UTC] Saving snapshot
[2019-11-06 07:54:56.458878 UTC] Starting iteration 43
[2019-11-06 07:54:56.459097 UTC] Start collecting samples
[2019-11-06 07:54:56.967913 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:57.009142 UTC] Computing policy gradient
[2019-11-06 07:54:57.030259 UTC] Updating baseline
[2019-11-06 07:54:57.237822 UTC] Computing logging information
-----------------------------------
| Iteration            | 43       |
| SurrLoss             | 0.023474 |
| Entropy              | 0.22058  |
| Perplexity           | 1.2468   |
| AveragePolicyProb[0] | 0.50365  |
| AveragePolicyProb[1] | 0.49636  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 576      |
| TotalNSamples        | 86543    |
| ExplainedVariance    | 0.39104  |
-----------------------------------
[2019-11-06 07:54:57.306438 UTC] Saving snapshot
[2019-11-06 07:54:57.324992 UTC] Starting iteration 44
[2019-11-06 07:54:57.325878 UTC] Start collecting samples
[2019-11-06 07:54:57.859704 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:57.890580 UTC] Computing policy gradient
[2019-11-06 07:54:57.906246 UTC] Updating baseline
[2019-11-06 07:54:58.079205 UTC] Computing logging information
------------------------------------
| Iteration            | 44        |
| SurrLoss             | 0.0061807 |
| Entropy              | 0.22472   |
| Perplexity           | 1.252     |
| AveragePolicyProb[0] | 0.50292   |
| AveragePolicyProb[1] | 0.49708   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 584       |
| TotalNSamples        | 88143     |
| ExplainedVariance    | 0.48984   |
------------------------------------
[2019-11-06 07:54:58.125931 UTC] Saving snapshot
[2019-11-06 07:54:58.137939 UTC] Starting iteration 45
[2019-11-06 07:54:58.138201 UTC] Start collecting samples
[2019-11-06 07:54:58.540214 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:58.579506 UTC] Computing policy gradient
[2019-11-06 07:54:58.595062 UTC] Updating baseline
[2019-11-06 07:54:58.802099 UTC] Computing logging information
------------------------------------
| Iteration            | 45        |
| SurrLoss             | 0.0047472 |
| Entropy              | 0.24152   |
| Perplexity           | 1.2732    |
| AveragePolicyProb[0] | 0.49427   |
| AveragePolicyProb[1] | 0.50573   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 596       |
| TotalNSamples        | 90543     |
| ExplainedVariance    | 0.28464   |
------------------------------------
[2019-11-06 07:54:58.858587 UTC] Saving snapshot
[2019-11-06 07:54:58.870834 UTC] Starting iteration 46
[2019-11-06 07:54:58.871107 UTC] Start collecting samples
[2019-11-06 07:54:59.340764 UTC] Computing input variables for policy optimization
[2019-11-06 07:54:59.375069 UTC] Computing policy gradient
[2019-11-06 07:54:59.395226 UTC] Updating baseline
[2019-11-06 07:54:59.625326 UTC] Computing logging information
------------------------------------
| Iteration            | 46        |
| SurrLoss             | -0.016625 |
| Entropy              | 0.25168   |
| Perplexity           | 1.2862    |
| AveragePolicyProb[0] | 0.50086   |
| AveragePolicyProb[1] | 0.49914   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 605       |
| TotalNSamples        | 92343     |
| ExplainedVariance    | 0.25357   |
------------------------------------
[2019-11-06 07:54:59.684617 UTC] Saving snapshot
[2019-11-06 07:54:59.698927 UTC] Starting iteration 47
[2019-11-06 07:54:59.699697 UTC] Start collecting samples
[2019-11-06 07:55:00.157278 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:00.205503 UTC] Computing policy gradient
[2019-11-06 07:55:00.223250 UTC] Updating baseline
[2019-11-06 07:55:00.437233 UTC] Computing logging information
------------------------------------
| Iteration            | 47        |
| SurrLoss             | -0.015742 |
| Entropy              | 0.25732   |
| Perplexity           | 1.2935    |
| AveragePolicyProb[0] | 0.49697   |
| AveragePolicyProb[1] | 0.50303   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 616       |
| TotalNSamples        | 94543     |
| ExplainedVariance    | 0.19877   |
------------------------------------
[2019-11-06 07:55:00.504764 UTC] Saving snapshot
[2019-11-06 07:55:00.519257 UTC] Starting iteration 48
[2019-11-06 07:55:00.520048 UTC] Start collecting samples
[2019-11-06 07:55:00.971411 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:01.004474 UTC] Computing policy gradient
[2019-11-06 07:55:01.019801 UTC] Updating baseline
[2019-11-06 07:55:01.209194 UTC] Computing logging information
--------------------------------------
| Iteration            | 48          |
| SurrLoss             | -0.00068493 |
| Entropy              | 0.25687     |
| Perplexity           | 1.2929      |
| AveragePolicyProb[0] | 0.50604     |
| AveragePolicyProb[1] | 0.49396     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 627         |
| TotalNSamples        | 96743       |
| ExplainedVariance    | 0.32934     |
--------------------------------------
[2019-11-06 07:55:01.273683 UTC] Saving snapshot
[2019-11-06 07:55:01.290023 UTC] Starting iteration 49
[2019-11-06 07:55:01.290272 UTC] Start collecting samples
[2019-11-06 07:55:01.671810 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:01.705947 UTC] Computing policy gradient
[2019-11-06 07:55:01.720131 UTC] Updating baseline
[2019-11-06 07:55:01.955533 UTC] Computing logging information
------------------------------------
| Iteration            | 49        |
| SurrLoss             | 0.0078327 |
| Entropy              | 0.27114   |
| Perplexity           | 1.3115    |
| AveragePolicyProb[0] | 0.49124   |
| AveragePolicyProb[1] | 0.50876   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 635       |
| TotalNSamples        | 98343     |
| ExplainedVariance    | 0.55039   |
------------------------------------
[2019-11-06 07:55:02.002129 UTC] Saving snapshot
[2019-11-06 07:55:02.015235 UTC] Starting iteration 50
[2019-11-06 07:55:02.015455 UTC] Start collecting samples
[2019-11-06 07:55:02.483735 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:02.521166 UTC] Computing policy gradient
[2019-11-06 07:55:02.535105 UTC] Updating baseline
[2019-11-06 07:55:02.751410 UTC] Computing logging information
-------------------------------------
| Iteration            | 50         |
| SurrLoss             | -0.014792  |
| Entropy              | 0.28492    |
| Perplexity           | 1.3297     |
| AveragePolicyProb[0] | 0.50422    |
| AveragePolicyProb[1] | 0.49578    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 644        |
| TotalNSamples        | 1.0014e+05 |
| ExplainedVariance    | 0.35474    |
-------------------------------------
[2019-11-06 07:55:02.809502 UTC] Saving snapshot
[2019-11-06 07:55:02.824922 UTC] Starting iteration 51
[2019-11-06 07:55:02.825177 UTC] Start collecting samples
[2019-11-06 07:55:03.316568 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:03.351730 UTC] Computing policy gradient
[2019-11-06 07:55:03.367569 UTC] Updating baseline
[2019-11-06 07:55:03.571691 UTC] Computing logging information
-------------------------------------
| Iteration            | 51         |
| SurrLoss             | 0.0054623  |
| Entropy              | 0.28651    |
| Perplexity           | 1.3318     |
| AveragePolicyProb[0] | 0.50818    |
| AveragePolicyProb[1] | 0.49182    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 656        |
| TotalNSamples        | 1.0254e+05 |
| ExplainedVariance    | 0.27268    |
-------------------------------------
[2019-11-06 07:55:03.629939 UTC] Saving snapshot
[2019-11-06 07:55:03.646287 UTC] Starting iteration 52
[2019-11-06 07:55:03.646958 UTC] Start collecting samples
[2019-11-06 07:55:04.083818 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:04.113202 UTC] Computing policy gradient
[2019-11-06 07:55:04.125165 UTC] Updating baseline
[2019-11-06 07:55:04.285307 UTC] Computing logging information
-------------------------------------
| Iteration            | 52         |
| SurrLoss             | -0.029588  |
| Entropy              | 0.30086    |
| Perplexity           | 1.351      |
| AveragePolicyProb[0] | 0.50746    |
| AveragePolicyProb[1] | 0.49254    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 664        |
| TotalNSamples        | 1.0414e+05 |
| ExplainedVariance    | 0.17376    |
-------------------------------------
[2019-11-06 07:55:04.345844 UTC] Saving snapshot
[2019-11-06 07:55:04.361456 UTC] Starting iteration 53
[2019-11-06 07:55:04.361710 UTC] Start collecting samples
[2019-11-06 07:55:04.849259 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:04.887006 UTC] Computing policy gradient
[2019-11-06 07:55:04.900712 UTC] Updating baseline
[2019-11-06 07:55:05.074125 UTC] Computing logging information
-------------------------------------
| Iteration            | 53         |
| SurrLoss             | -0.015965  |
| Entropy              | 0.29288    |
| Perplexity           | 1.3403     |
| AveragePolicyProb[0] | 0.4939     |
| AveragePolicyProb[1] | 0.5061     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 676        |
| TotalNSamples        | 1.0654e+05 |
| ExplainedVariance    | 0.19981    |
-------------------------------------
[2019-11-06 07:55:05.132412 UTC] Saving snapshot
[2019-11-06 07:55:05.144675 UTC] Starting iteration 54
[2019-11-06 07:55:05.144869 UTC] Start collecting samples
[2019-11-06 07:55:05.622914 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:05.655513 UTC] Computing policy gradient
[2019-11-06 07:55:05.671373 UTC] Updating baseline
[2019-11-06 07:55:05.904138 UTC] Computing logging information
-------------------------------------
| Iteration            | 54         |
| SurrLoss             | -0.0065934 |
| Entropy              | 0.28847    |
| Perplexity           | 1.3344     |
| AveragePolicyProb[0] | 0.48555    |
| AveragePolicyProb[1] | 0.51445    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 685        |
| TotalNSamples        | 1.0834e+05 |
| ExplainedVariance    | 0.33892    |
-------------------------------------
[2019-11-06 07:55:05.952070 UTC] Saving snapshot
[2019-11-06 07:55:05.963545 UTC] Starting iteration 55
[2019-11-06 07:55:05.963772 UTC] Start collecting samples
[2019-11-06 07:55:06.304758 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:06.334653 UTC] Computing policy gradient
[2019-11-06 07:55:06.348012 UTC] Updating baseline
[2019-11-06 07:55:06.529298 UTC] Computing logging information
-------------------------------------
| Iteration            | 55         |
| SurrLoss             | 0.0058793  |
| Entropy              | 0.29937    |
| Perplexity           | 1.349      |
| AveragePolicyProb[0] | 0.51262    |
| AveragePolicyProb[1] | 0.48738    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 696        |
| TotalNSamples        | 1.1054e+05 |
| ExplainedVariance    | 0.092993   |
-------------------------------------
[2019-11-06 07:55:06.576827 UTC] Saving snapshot
[2019-11-06 07:55:06.585767 UTC] Starting iteration 56
[2019-11-06 07:55:06.585978 UTC] Start collecting samples
[2019-11-06 07:55:06.932221 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:06.961356 UTC] Computing policy gradient
[2019-11-06 07:55:06.972121 UTC] Updating baseline
[2019-11-06 07:55:07.159995 UTC] Computing logging information
-------------------------------------
| Iteration            | 56         |
| SurrLoss             | -0.0049614 |
| Entropy              | 0.29723    |
| Perplexity           | 1.3461     |
| AveragePolicyProb[0] | 0.48553    |
| AveragePolicyProb[1] | 0.51447    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 707        |
| TotalNSamples        | 1.1274e+05 |
| ExplainedVariance    | 0.31305    |
-------------------------------------
[2019-11-06 07:55:07.205251 UTC] Saving snapshot
[2019-11-06 07:55:07.216378 UTC] Starting iteration 57
[2019-11-06 07:55:07.216591 UTC] Start collecting samples
[2019-11-06 07:55:07.525964 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:07.550640 UTC] Computing policy gradient
[2019-11-06 07:55:07.563658 UTC] Updating baseline
[2019-11-06 07:55:07.767005 UTC] Computing logging information
-------------------------------------
| Iteration            | 57         |
| SurrLoss             | -0.0026451 |
| Entropy              | 0.3004     |
| Perplexity           | 1.3504     |
| AveragePolicyProb[0] | 0.50657    |
| AveragePolicyProb[1] | 0.49343    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 715        |
| TotalNSamples        | 1.1434e+05 |
| ExplainedVariance    | 0.29851    |
-------------------------------------
[2019-11-06 07:55:07.814623 UTC] Saving snapshot
[2019-11-06 07:55:07.825773 UTC] Starting iteration 58
[2019-11-06 07:55:07.825963 UTC] Start collecting samples
[2019-11-06 07:55:08.056191 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:08.075132 UTC] Computing policy gradient
[2019-11-06 07:55:08.082827 UTC] Updating baseline
[2019-11-06 07:55:08.187508 UTC] Computing logging information
-------------------------------------
| Iteration            | 58         |
| SurrLoss             | -0.033169  |
| Entropy              | 0.30783    |
| Perplexity           | 1.3605     |
| AveragePolicyProb[0] | 0.50619    |
| AveragePolicyProb[1] | 0.49381    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 724        |
| TotalNSamples        | 1.1614e+05 |
| ExplainedVariance    | 0.37634    |
-------------------------------------
[2019-11-06 07:55:08.220982 UTC] Saving snapshot
[2019-11-06 07:55:08.227613 UTC] Starting iteration 59
[2019-11-06 07:55:08.227784 UTC] Start collecting samples
[2019-11-06 07:55:08.466334 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:08.484405 UTC] Computing policy gradient
[2019-11-06 07:55:08.495029 UTC] Updating baseline
[2019-11-06 07:55:08.624489 UTC] Computing logging information
-------------------------------------
| Iteration            | 59         |
| SurrLoss             | 0.01208    |
| Entropy              | 0.29629    |
| Perplexity           | 1.3449     |
| AveragePolicyProb[0] | 0.50863    |
| AveragePolicyProb[1] | 0.49137    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 736        |
| TotalNSamples        | 1.1854e+05 |
| ExplainedVariance    | 0.44955    |
-------------------------------------
[2019-11-06 07:55:08.659034 UTC] Saving snapshot
[2019-11-06 07:55:08.665543 UTC] Starting iteration 60
[2019-11-06 07:55:08.665709 UTC] Start collecting samples
[2019-11-06 07:55:08.936532 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:08.965805 UTC] Computing policy gradient
[2019-11-06 07:55:08.977705 UTC] Updating baseline
[2019-11-06 07:55:09.150755 UTC] Computing logging information
-------------------------------------
| Iteration            | 60         |
| SurrLoss             | 0.011792   |
| Entropy              | 0.30345    |
| Perplexity           | 1.3545     |
| AveragePolicyProb[0] | 0.49837    |
| AveragePolicyProb[1] | 0.50163    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 744        |
| TotalNSamples        | 1.2014e+05 |
| ExplainedVariance    | 0.67098    |
-------------------------------------
[2019-11-06 07:55:09.192774 UTC] Saving snapshot
[2019-11-06 07:55:09.202801 UTC] Starting iteration 61
[2019-11-06 07:55:09.203033 UTC] Start collecting samples
[2019-11-06 07:55:09.587433 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:09.621048 UTC] Computing policy gradient
[2019-11-06 07:55:09.634458 UTC] Updating baseline
[2019-11-06 07:55:09.823084 UTC] Computing logging information
-------------------------------------
| Iteration            | 61         |
| SurrLoss             | -0.004161  |
| Entropy              | 0.32662    |
| Perplexity           | 1.3863     |
| AveragePolicyProb[0] | 0.51158    |
| AveragePolicyProb[1] | 0.48842    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 756        |
| TotalNSamples        | 1.2254e+05 |
| ExplainedVariance    | 0.74187    |
-------------------------------------
[2019-11-06 07:55:09.866822 UTC] Saving snapshot
[2019-11-06 07:55:09.875804 UTC] Starting iteration 62
[2019-11-06 07:55:09.876061 UTC] Start collecting samples
[2019-11-06 07:55:10.186282 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:10.220522 UTC] Computing policy gradient
[2019-11-06 07:55:10.233115 UTC] Updating baseline
[2019-11-06 07:55:10.418849 UTC] Computing logging information
-------------------------------------
| Iteration            | 62         |
| SurrLoss             | -0.010567  |
| Entropy              | 0.33196    |
| Perplexity           | 1.3937     |
| AveragePolicyProb[0] | 0.5016     |
| AveragePolicyProb[1] | 0.4984     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 765        |
| TotalNSamples        | 1.2434e+05 |
| ExplainedVariance    | 0.73904    |
-------------------------------------
[2019-11-06 07:55:10.463638 UTC] Saving snapshot
[2019-11-06 07:55:10.492859 UTC] Starting iteration 63
[2019-11-06 07:55:10.493119 UTC] Start collecting samples
[2019-11-06 07:55:10.917331 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:10.948445 UTC] Computing policy gradient
[2019-11-06 07:55:10.962201 UTC] Updating baseline
[2019-11-06 07:55:11.154763 UTC] Computing logging information
-------------------------------------
| Iteration            | 63         |
| SurrLoss             | -0.0084199 |
| Entropy              | 0.32799    |
| Perplexity           | 1.3882     |
| AveragePolicyProb[0] | 0.50785    |
| AveragePolicyProb[1] | 0.49215    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 776        |
| TotalNSamples        | 1.2654e+05 |
| ExplainedVariance    | 0.60613    |
-------------------------------------
[2019-11-06 07:55:11.209510 UTC] Saving snapshot
[2019-11-06 07:55:11.221599 UTC] Starting iteration 64
[2019-11-06 07:55:11.222341 UTC] Start collecting samples
[2019-11-06 07:55:11.607863 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:11.643028 UTC] Computing policy gradient
[2019-11-06 07:55:11.658909 UTC] Updating baseline
[2019-11-06 07:55:11.881212 UTC] Computing logging information
-------------------------------------
| Iteration            | 64         |
| SurrLoss             | -0.015793  |
| Entropy              | 0.3336     |
| Perplexity           | 1.396      |
| AveragePolicyProb[0] | 0.50966    |
| AveragePolicyProb[1] | 0.49034    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 787        |
| TotalNSamples        | 1.2874e+05 |
| ExplainedVariance    | 0.58451    |
-------------------------------------
[2019-11-06 07:55:11.947711 UTC] Saving snapshot
[2019-11-06 07:55:11.960661 UTC] Starting iteration 65
[2019-11-06 07:55:11.960912 UTC] Start collecting samples
[2019-11-06 07:55:12.633217 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:12.659025 UTC] Computing policy gradient
[2019-11-06 07:55:12.672373 UTC] Updating baseline
[2019-11-06 07:55:12.883487 UTC] Computing logging information
-------------------------------------
| Iteration            | 65         |
| SurrLoss             | 0.0030379  |
| Entropy              | 0.33822    |
| Perplexity           | 1.4025     |
| AveragePolicyProb[0] | 0.48834    |
| AveragePolicyProb[1] | 0.51166    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 795        |
| TotalNSamples        | 1.3034e+05 |
| ExplainedVariance    | 0.43733    |
-------------------------------------
[2019-11-06 07:55:12.933493 UTC] Saving snapshot
[2019-11-06 07:55:12.942616 UTC] Starting iteration 66
[2019-11-06 07:55:12.942820 UTC] Start collecting samples
[2019-11-06 07:55:13.268603 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:13.296983 UTC] Computing policy gradient
[2019-11-06 07:55:13.307919 UTC] Updating baseline
[2019-11-06 07:55:13.497188 UTC] Computing logging information
-------------------------------------
| Iteration            | 66         |
| SurrLoss             | 0.0055713  |
| Entropy              | 0.34191    |
| Perplexity           | 1.4076     |
| AveragePolicyProb[0] | 0.50055    |
| AveragePolicyProb[1] | 0.49945    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 804        |
| TotalNSamples        | 1.3214e+05 |
| ExplainedVariance    | 0.44698    |
-------------------------------------
[2019-11-06 07:55:13.547563 UTC] Saving snapshot
[2019-11-06 07:55:13.556920 UTC] Starting iteration 67
[2019-11-06 07:55:13.557173 UTC] Start collecting samples
[2019-11-06 07:55:13.882153 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:13.915295 UTC] Computing policy gradient
[2019-11-06 07:55:13.931074 UTC] Updating baseline
[2019-11-06 07:55:14.135622 UTC] Computing logging information
--------------------------------------
| Iteration            | 67          |
| SurrLoss             | -0.00046859 |
| Entropy              | 0.35142     |
| Perplexity           | 1.4211      |
| AveragePolicyProb[0] | 0.50166     |
| AveragePolicyProb[1] | 0.49834     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 816         |
| TotalNSamples        | 1.3454e+05  |
| ExplainedVariance    | 0.22474     |
--------------------------------------
[2019-11-06 07:55:14.195819 UTC] Saving snapshot
[2019-11-06 07:55:14.205631 UTC] Starting iteration 68
[2019-11-06 07:55:14.205840 UTC] Start collecting samples
[2019-11-06 07:55:14.521329 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:14.550020 UTC] Computing policy gradient
[2019-11-06 07:55:14.563300 UTC] Updating baseline
[2019-11-06 07:55:14.760032 UTC] Computing logging information
-------------------------------------
| Iteration            | 68         |
| SurrLoss             | 0.0061919  |
| Entropy              | 0.37468    |
| Perplexity           | 1.4545     |
| AveragePolicyProb[0] | 0.5017     |
| AveragePolicyProb[1] | 0.4983     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 824        |
| TotalNSamples        | 1.3614e+05 |
| ExplainedVariance    | 0.2511     |
-------------------------------------
[2019-11-06 07:55:14.807012 UTC] Saving snapshot
[2019-11-06 07:55:14.817078 UTC] Starting iteration 69
[2019-11-06 07:55:14.817426 UTC] Start collecting samples
[2019-11-06 07:55:15.175101 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:15.210740 UTC] Computing policy gradient
[2019-11-06 07:55:15.226222 UTC] Updating baseline
[2019-11-06 07:55:15.404014 UTC] Computing logging information
-------------------------------------
| Iteration            | 69         |
| SurrLoss             | 0.016379   |
| Entropy              | 0.39508    |
| Perplexity           | 1.4845     |
| AveragePolicyProb[0] | 0.4992     |
| AveragePolicyProb[1] | 0.5008     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 836        |
| TotalNSamples        | 1.3854e+05 |
| ExplainedVariance    | 0.013234   |
-------------------------------------
[2019-11-06 07:55:15.452564 UTC] Saving snapshot
[2019-11-06 07:55:15.462728 UTC] Starting iteration 70
[2019-11-06 07:55:15.462944 UTC] Start collecting samples
[2019-11-06 07:55:15.806001 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:15.833077 UTC] Computing policy gradient
[2019-11-06 07:55:15.846032 UTC] Updating baseline
[2019-11-06 07:55:16.073326 UTC] Computing logging information
-------------------------------------
| Iteration            | 70         |
| SurrLoss             | 0.008201   |
| Entropy              | 0.43069    |
| Perplexity           | 1.5383     |
| AveragePolicyProb[0] | 0.50651    |
| AveragePolicyProb[1] | 0.49349    |
| AverageReturn        | 198.67     |
| MinReturn            | 101        |
| MaxReturn            | 200        |
| StdReturn            | 10.383     |
| AverageEpisodeLength | 198.67     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 10.383     |
| TotalNEpisodes       | 846        |
| TotalNSamples        | 1.4041e+05 |
| ExplainedVariance    | 0.39768    |
-------------------------------------
[2019-11-06 07:55:16.125000 UTC] Saving snapshot
[2019-11-06 07:55:16.134677 UTC] Starting iteration 71
[2019-11-06 07:55:16.134904 UTC] Start collecting samples
[2019-11-06 07:55:16.524228 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:16.576769 UTC] Computing policy gradient
[2019-11-06 07:55:16.590862 UTC] Updating baseline
[2019-11-06 07:55:16.763409 UTC] Computing logging information
-------------------------------------
| Iteration            | 71         |
| SurrLoss             | 0.045265   |
| Entropy              | 0.37878    |
| Perplexity           | 1.4605     |
| AveragePolicyProb[0] | 0.54676    |
| AveragePolicyProb[1] | 0.45324    |
| AverageReturn        | 143.07     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 77.978     |
| AverageEpisodeLength | 143.07     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 77.978     |
| TotalNEpisodes       | 889        |
| TotalNSamples        | 1.4345e+05 |
| ExplainedVariance    | -0.22732   |
-------------------------------------
[2019-11-06 07:55:16.811954 UTC] Saving snapshot
[2019-11-06 07:55:16.822185 UTC] Starting iteration 72
[2019-11-06 07:55:16.822402 UTC] Start collecting samples
[2019-11-06 07:55:17.158886 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:17.196422 UTC] Computing policy gradient
[2019-11-06 07:55:17.211326 UTC] Updating baseline
[2019-11-06 07:55:17.414065 UTC] Computing logging information
-------------------------------------
| Iteration            | 72         |
| SurrLoss             | 0.032453   |
| Entropy              | 0.38764    |
| Perplexity           | 1.4735     |
| AveragePolicyProb[0] | 0.49079    |
| AveragePolicyProb[1] | 0.50921    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 896        |
| TotalNSamples        | 1.4405e+05 |
| ExplainedVariance    | 0.46846    |
-------------------------------------
[2019-11-06 07:55:17.476370 UTC] Saving snapshot
[2019-11-06 07:55:17.490122 UTC] Starting iteration 73
[2019-11-06 07:55:17.490567 UTC] Start collecting samples
[2019-11-06 07:55:17.926834 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:17.960279 UTC] Computing policy gradient
[2019-11-06 07:55:17.974566 UTC] Updating baseline
[2019-11-06 07:55:18.166160 UTC] Computing logging information
-------------------------------------
| Iteration            | 73         |
| SurrLoss             | -0.0044491 |
| Entropy              | 0.35521    |
| Perplexity           | 1.4265     |
| AveragePolicyProb[0] | 0.49371    |
| AveragePolicyProb[1] | 0.50629    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 909        |
| TotalNSamples        | 1.4665e+05 |
| ExplainedVariance    | 0.73063    |
-------------------------------------
[2019-11-06 07:55:18.218196 UTC] Saving snapshot
[2019-11-06 07:55:18.230444 UTC] Starting iteration 74
[2019-11-06 07:55:18.230907 UTC] Start collecting samples
[2019-11-06 07:55:18.552786 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:18.577978 UTC] Computing policy gradient
[2019-11-06 07:55:18.591814 UTC] Updating baseline
[2019-11-06 07:55:18.776325 UTC] Computing logging information
-------------------------------------
| Iteration            | 74         |
| SurrLoss             | -0.0023291 |
| Entropy              | 0.32163    |
| Perplexity           | 1.3794     |
| AveragePolicyProb[0] | 0.49411    |
| AveragePolicyProb[1] | 0.50589    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 916        |
| TotalNSamples        | 1.4805e+05 |
| ExplainedVariance    | 0.8342     |
-------------------------------------
[2019-11-06 07:55:18.831359 UTC] Saving snapshot
[2019-11-06 07:55:18.841817 UTC] Starting iteration 75
[2019-11-06 07:55:18.842048 UTC] Start collecting samples
[2019-11-06 07:55:19.208840 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:19.242027 UTC] Computing policy gradient
[2019-11-06 07:55:19.253886 UTC] Updating baseline
[2019-11-06 07:55:19.422485 UTC] Computing logging information
-------------------------------------
| Iteration            | 75         |
| SurrLoss             | -0.010632  |
| Entropy              | 0.27663    |
| Perplexity           | 1.3187     |
| AveragePolicyProb[0] | 0.51146    |
| AveragePolicyProb[1] | 0.48854    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 927        |
| TotalNSamples        | 1.5025e+05 |
| ExplainedVariance    | 0.8223     |
-------------------------------------
[2019-11-06 07:55:19.474851 UTC] Saving snapshot
[2019-11-06 07:55:19.486784 UTC] Starting iteration 76
[2019-11-06 07:55:19.487452 UTC] Start collecting samples
[2019-11-06 07:55:19.867625 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:19.906366 UTC] Computing policy gradient
[2019-11-06 07:55:19.918254 UTC] Updating baseline
[2019-11-06 07:55:20.059682 UTC] Computing logging information
-------------------------------------
| Iteration            | 76         |
| SurrLoss             | -0.0065858 |
| Entropy              | 0.25984    |
| Perplexity           | 1.2967     |
| AveragePolicyProb[0] | 0.49691    |
| AveragePolicyProb[1] | 0.50309    |
| AverageReturn        | 136.02     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.262     |
| AverageEpisodeLength | 136.02     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.262     |
| TotalNEpisodes       | 941        |
| TotalNSamples        | 1.5305e+05 |
| ExplainedVariance    | 0.75918    |
-------------------------------------
[2019-11-06 07:55:20.108338 UTC] Saving snapshot
[2019-11-06 07:55:20.118730 UTC] Starting iteration 77
[2019-11-06 07:55:20.118960 UTC] Start collecting samples
[2019-11-06 07:55:20.465423 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:20.487640 UTC] Computing policy gradient
[2019-11-06 07:55:20.499770 UTC] Updating baseline
[2019-11-06 07:55:20.705949 UTC] Computing logging information
-------------------------------------
| Iteration            | 77         |
| SurrLoss             | -0.0041449 |
| Entropy              | 0.23782    |
| Perplexity           | 1.2685     |
| AveragePolicyProb[0] | 0.50053    |
| AveragePolicyProb[1] | 0.49947    |
| AverageReturn        | 136.36     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.462     |
| AverageEpisodeLength | 136.36     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.462     |
| TotalNEpisodes       | 945        |
| TotalNSamples        | 1.5385e+05 |
| ExplainedVariance    | 0.66709    |
-------------------------------------
[2019-11-06 07:55:20.758289 UTC] Saving snapshot
[2019-11-06 07:55:20.769638 UTC] Starting iteration 78
[2019-11-06 07:55:20.769870 UTC] Start collecting samples
[2019-11-06 07:55:21.159303 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:21.198624 UTC] Computing policy gradient
[2019-11-06 07:55:21.212854 UTC] Updating baseline
[2019-11-06 07:55:21.411704 UTC] Computing logging information
-------------------------------------
| Iteration            | 78         |
| SurrLoss             | 0.0011773  |
| Entropy              | 0.22558    |
| Perplexity           | 1.2531     |
| AveragePolicyProb[0] | 0.49331    |
| AveragePolicyProb[1] | 0.50669    |
| AverageReturn        | 146.62     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 76.754     |
| AverageEpisodeLength | 146.62     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 76.754     |
| TotalNEpisodes       | 958        |
| TotalNSamples        | 1.5645e+05 |
| ExplainedVariance    | 0.43945    |
-------------------------------------
[2019-11-06 07:55:21.471927 UTC] Saving snapshot
[2019-11-06 07:55:21.484629 UTC] Starting iteration 79
[2019-11-06 07:55:21.484880 UTC] Start collecting samples
[2019-11-06 07:55:21.888619 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:21.920071 UTC] Computing policy gradient
[2019-11-06 07:55:21.934213 UTC] Updating baseline
[2019-11-06 07:55:22.122968 UTC] Computing logging information
-------------------------------------
| Iteration            | 79         |
| SurrLoss             | -0.0034739 |
| Entropy              | 0.23803    |
| Perplexity           | 1.2687     |
| AveragePolicyProb[0] | 0.50235    |
| AveragePolicyProb[1] | 0.49765    |
| AverageReturn        | 163.81     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 68.162     |
| AverageEpisodeLength | 163.81     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 68.162     |
| TotalNEpisodes       | 971        |
| TotalNSamples        | 1.5905e+05 |
| ExplainedVariance    | 0.2124     |
-------------------------------------
[2019-11-06 07:55:22.186946 UTC] Saving snapshot
[2019-11-06 07:55:22.199484 UTC] Starting iteration 80
[2019-11-06 07:55:22.199730 UTC] Start collecting samples
[2019-11-06 07:55:22.546577 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:22.570633 UTC] Computing policy gradient
[2019-11-06 07:55:22.583592 UTC] Updating baseline
[2019-11-06 07:55:22.784732 UTC] Computing logging information
--------------------------------------
| Iteration            | 80          |
| SurrLoss             | -0.00021024 |
| Entropy              | 0.20728     |
| Perplexity           | 1.2303      |
| AveragePolicyProb[0] | 0.50687     |
| AveragePolicyProb[1] | 0.49313     |
| AverageReturn        | 172.85      |
| MinReturn            | 10          |
| MaxReturn            | 200         |
| StdReturn            | 59.749      |
| AverageEpisodeLength | 172.85      |
| MinEpisodeLength     | 10          |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 59.749      |
| TotalNEpisodes       | 976         |
| TotalNSamples        | 1.6005e+05  |
| ExplainedVariance    | 0.14898     |
--------------------------------------
[2019-11-06 07:55:22.839107 UTC] Saving snapshot
[2019-11-06 07:55:22.851383 UTC] Starting iteration 81
[2019-11-06 07:55:22.851612 UTC] Start collecting samples
[2019-11-06 07:55:23.164149 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:23.195765 UTC] Computing policy gradient
[2019-11-06 07:55:23.209306 UTC] Updating baseline
[2019-11-06 07:55:23.419499 UTC] Computing logging information
-------------------------------------
| Iteration            | 81         |
| SurrLoss             | -0.0025101 |
| Entropy              | 0.19662    |
| Perplexity           | 1.2173     |
| AveragePolicyProb[0] | 0.49724    |
| AveragePolicyProb[1] | 0.50276    |
| AverageReturn        | 191.96     |
| MinReturn            | 12         |
| MaxReturn            | 200        |
| StdReturn            | 32.978     |
| AverageEpisodeLength | 191.96     |
| MinEpisodeLength     | 12         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 32.978     |
| TotalNEpisodes       | 989        |
| TotalNSamples        | 1.6265e+05 |
| ExplainedVariance    | -0.11324   |
-------------------------------------
[2019-11-06 07:55:23.473484 UTC] Saving snapshot
[2019-11-06 07:55:23.485562 UTC] Starting iteration 82
[2019-11-06 07:55:23.485832 UTC] Start collecting samples
[2019-11-06 07:55:23.886525 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:23.921516 UTC] Computing policy gradient
[2019-11-06 07:55:23.935304 UTC] Updating baseline
[2019-11-06 07:55:24.164849 UTC] Computing logging information
-------------------------------------
| Iteration            | 82         |
| SurrLoss             | -0.0071702 |
| Entropy              | 0.21372    |
| Perplexity           | 1.2383     |
| AveragePolicyProb[0] | 0.49627    |
| AveragePolicyProb[1] | 0.50373    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 996        |
| TotalNSamples        | 1.6405e+05 |
| ExplainedVariance    | 0.49696    |
-------------------------------------
[2019-11-06 07:55:24.238119 UTC] Saving snapshot
[2019-11-06 07:55:24.252497 UTC] Starting iteration 83
[2019-11-06 07:55:24.253313 UTC] Start collecting samples
[2019-11-06 07:55:24.684912 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:24.731318 UTC] Computing policy gradient
[2019-11-06 07:55:24.746804 UTC] Updating baseline
[2019-11-06 07:55:24.985661 UTC] Computing logging information
-------------------------------------
| Iteration            | 83         |
| SurrLoss             | 0.017254   |
| Entropy              | 0.18241    |
| Perplexity           | 1.2001     |
| AveragePolicyProb[0] | 0.51173    |
| AveragePolicyProb[1] | 0.48827    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1007       |
| TotalNSamples        | 1.6625e+05 |
| ExplainedVariance    | 0.44712    |
-------------------------------------
[2019-11-06 07:55:25.060071 UTC] Saving snapshot
[2019-11-06 07:55:25.074183 UTC] Starting iteration 84
[2019-11-06 07:55:25.074966 UTC] Start collecting samples
[2019-11-06 07:55:25.514922 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:25.546377 UTC] Computing policy gradient
[2019-11-06 07:55:25.559056 UTC] Updating baseline
[2019-11-06 07:55:25.746771 UTC] Computing logging information
-------------------------------------
| Iteration            | 84         |
| SurrLoss             | 0.015674   |
| Entropy              | 0.18013    |
| Perplexity           | 1.1974     |
| AveragePolicyProb[0] | 0.50674    |
| AveragePolicyProb[1] | 0.49326    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1021       |
| TotalNSamples        | 1.6905e+05 |
| ExplainedVariance    | 0.70126    |
-------------------------------------
[2019-11-06 07:55:25.800267 UTC] Saving snapshot
[2019-11-06 07:55:25.812125 UTC] Starting iteration 85
[2019-11-06 07:55:25.812356 UTC] Start collecting samples
[2019-11-06 07:55:26.177510 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:26.200933 UTC] Computing policy gradient
[2019-11-06 07:55:26.213269 UTC] Updating baseline
[2019-11-06 07:55:26.393754 UTC] Computing logging information
-------------------------------------
| Iteration            | 85         |
| SurrLoss             | 0.0063141  |
| Entropy              | 0.1852     |
| Perplexity           | 1.2035     |
| AveragePolicyProb[0] | 0.50028    |
| AveragePolicyProb[1] | 0.49972    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1025       |
| TotalNSamples        | 1.6985e+05 |
| ExplainedVariance    | 0.6582     |
-------------------------------------
[2019-11-06 07:55:26.447432 UTC] Saving snapshot
[2019-11-06 07:55:26.459149 UTC] Starting iteration 86
[2019-11-06 07:55:26.459352 UTC] Start collecting samples
[2019-11-06 07:55:26.863090 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:26.900082 UTC] Computing policy gradient
[2019-11-06 07:55:26.914351 UTC] Updating baseline
[2019-11-06 07:55:27.089648 UTC] Computing logging information
-------------------------------------
| Iteration            | 86         |
| SurrLoss             | -0.013808  |
| Entropy              | 0.17897    |
| Perplexity           | 1.196      |
| AveragePolicyProb[0] | 0.49889    |
| AveragePolicyProb[1] | 0.50111    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1038       |
| TotalNSamples        | 1.7245e+05 |
| ExplainedVariance    | 0.79214    |
-------------------------------------
[2019-11-06 07:55:27.138516 UTC] Saving snapshot
[2019-11-06 07:55:27.147530 UTC] Starting iteration 87
[2019-11-06 07:55:27.147737 UTC] Start collecting samples
[2019-11-06 07:55:27.523500 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:27.556094 UTC] Computing policy gradient
[2019-11-06 07:55:27.567678 UTC] Updating baseline
[2019-11-06 07:55:27.764345 UTC] Computing logging information
-------------------------------------
| Iteration            | 87         |
| SurrLoss             | 0.007005   |
| Entropy              | 0.18276    |
| Perplexity           | 1.2005     |
| AveragePolicyProb[0] | 0.49716    |
| AveragePolicyProb[1] | 0.50284    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1051       |
| TotalNSamples        | 1.7505e+05 |
| ExplainedVariance    | 0.55669    |
-------------------------------------
[2019-11-06 07:55:27.828110 UTC] Saving snapshot
[2019-11-06 07:55:27.840705 UTC] Starting iteration 88
[2019-11-06 07:55:27.840981 UTC] Start collecting samples
[2019-11-06 07:55:28.213227 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:28.245139 UTC] Computing policy gradient
[2019-11-06 07:55:28.260130 UTC] Updating baseline
[2019-11-06 07:55:28.470697 UTC] Computing logging information
-------------------------------------
| Iteration            | 88         |
| SurrLoss             | 0.0073207  |
| Entropy              | 0.16911    |
| Perplexity           | 1.1842     |
| AveragePolicyProb[0] | 0.50635    |
| AveragePolicyProb[1] | 0.49365    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1056       |
| TotalNSamples        | 1.7605e+05 |
| ExplainedVariance    | 0.67203    |
-------------------------------------
[2019-11-06 07:55:28.525227 UTC] Saving snapshot
[2019-11-06 07:55:28.536561 UTC] Starting iteration 89
[2019-11-06 07:55:28.536956 UTC] Start collecting samples
[2019-11-06 07:55:28.978528 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:29.020495 UTC] Computing policy gradient
[2019-11-06 07:55:29.033337 UTC] Updating baseline
[2019-11-06 07:55:29.244646 UTC] Computing logging information
-------------------------------------
| Iteration            | 89         |
| SurrLoss             | -0.013039  |
| Entropy              | 0.15615    |
| Perplexity           | 1.169      |
| AveragePolicyProb[0] | 0.50077    |
| AveragePolicyProb[1] | 0.49923    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1069       |
| TotalNSamples        | 1.7865e+05 |
| ExplainedVariance    | 0.66265    |
-------------------------------------
[2019-11-06 07:55:29.316391 UTC] Saving snapshot
[2019-11-06 07:55:29.329824 UTC] Starting iteration 90
[2019-11-06 07:55:29.330528 UTC] Start collecting samples
[2019-11-06 07:55:29.783231 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:29.813850 UTC] Computing policy gradient
[2019-11-06 07:55:29.827719 UTC] Updating baseline
[2019-11-06 07:55:30.036261 UTC] Computing logging information
-------------------------------------
| Iteration            | 90         |
| SurrLoss             | 0.029525   |
| Entropy              | 0.16489    |
| Perplexity           | 1.1793     |
| AveragePolicyProb[0] | 0.49603    |
| AveragePolicyProb[1] | 0.50397    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1076       |
| TotalNSamples        | 1.8005e+05 |
| ExplainedVariance    | 0.65342    |
-------------------------------------
[2019-11-06 07:55:30.106390 UTC] Saving snapshot
[2019-11-06 07:55:30.120849 UTC] Starting iteration 91
[2019-11-06 07:55:30.121089 UTC] Start collecting samples
[2019-11-06 07:55:30.557225 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:30.591130 UTC] Computing policy gradient
[2019-11-06 07:55:30.604975 UTC] Updating baseline
[2019-11-06 07:55:30.823808 UTC] Computing logging information
-------------------------------------
| Iteration            | 91         |
| SurrLoss             | -0.0032053 |
| Entropy              | 0.16088    |
| Perplexity           | 1.1745     |
| AveragePolicyProb[0] | 0.49554    |
| AveragePolicyProb[1] | 0.50446    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1087       |
| TotalNSamples        | 1.8225e+05 |
| ExplainedVariance    | 0.67526    |
-------------------------------------
[2019-11-06 07:55:30.899319 UTC] Saving snapshot
[2019-11-06 07:55:30.919912 UTC] Starting iteration 92
[2019-11-06 07:55:30.920410 UTC] Start collecting samples
[2019-11-06 07:55:31.334796 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:31.365171 UTC] Computing policy gradient
[2019-11-06 07:55:31.377671 UTC] Updating baseline
[2019-11-06 07:55:31.559633 UTC] Computing logging information
-------------------------------------
| Iteration            | 92         |
| SurrLoss             | -0.012856  |
| Entropy              | 0.14707    |
| Perplexity           | 1.1584     |
| AveragePolicyProb[0] | 0.49622    |
| AveragePolicyProb[1] | 0.50378    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1101       |
| TotalNSamples        | 1.8505e+05 |
| ExplainedVariance    | 0.50678    |
-------------------------------------
[2019-11-06 07:55:31.617661 UTC] Saving snapshot
[2019-11-06 07:55:31.628159 UTC] Starting iteration 93
[2019-11-06 07:55:31.628381 UTC] Start collecting samples
[2019-11-06 07:55:32.034950 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:32.057757 UTC] Computing policy gradient
[2019-11-06 07:55:32.070669 UTC] Updating baseline
[2019-11-06 07:55:32.264021 UTC] Computing logging information
-------------------------------------
| Iteration            | 93         |
| SurrLoss             | 0.016223   |
| Entropy              | 0.1644     |
| Perplexity           | 1.1787     |
| AveragePolicyProb[0] | 0.49241    |
| AveragePolicyProb[1] | 0.50759    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1105       |
| TotalNSamples        | 1.8585e+05 |
| ExplainedVariance    | 0.54041    |
-------------------------------------
[2019-11-06 07:55:32.323774 UTC] Saving snapshot
[2019-11-06 07:55:32.336275 UTC] Starting iteration 94
[2019-11-06 07:55:32.336494 UTC] Start collecting samples
[2019-11-06 07:55:32.708295 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:32.740771 UTC] Computing policy gradient
[2019-11-06 07:55:32.754545 UTC] Updating baseline
[2019-11-06 07:55:32.982468 UTC] Computing logging information
-------------------------------------
| Iteration            | 94         |
| SurrLoss             | -0.010354  |
| Entropy              | 0.14803    |
| Perplexity           | 1.1595     |
| AveragePolicyProb[0] | 0.5109     |
| AveragePolicyProb[1] | 0.4891     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1118       |
| TotalNSamples        | 1.8845e+05 |
| ExplainedVariance    | 0.15993    |
-------------------------------------
[2019-11-06 07:55:33.041029 UTC] Saving snapshot
[2019-11-06 07:55:33.052909 UTC] Starting iteration 95
[2019-11-06 07:55:33.053146 UTC] Start collecting samples
[2019-11-06 07:55:33.445683 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:33.473690 UTC] Computing policy gradient
[2019-11-06 07:55:33.486173 UTC] Updating baseline
[2019-11-06 07:55:33.671130 UTC] Computing logging information
-------------------------------------
| Iteration            | 95         |
| SurrLoss             | 0.012306   |
| Entropy              | 0.15255    |
| Perplexity           | 1.1648     |
| AveragePolicyProb[0] | 0.50327    |
| AveragePolicyProb[1] | 0.49673    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1131       |
| TotalNSamples        | 1.9105e+05 |
| ExplainedVariance    | 0.45237    |
-------------------------------------
[2019-11-06 07:55:33.727462 UTC] Saving snapshot
[2019-11-06 07:55:33.739111 UTC] Starting iteration 96
[2019-11-06 07:55:33.739338 UTC] Start collecting samples
[2019-11-06 07:55:34.223452 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:34.251412 UTC] Computing policy gradient
[2019-11-06 07:55:34.265550 UTC] Updating baseline
[2019-11-06 07:55:34.467942 UTC] Computing logging information
-------------------------------------
| Iteration            | 96         |
| SurrLoss             | -0.0063822 |
| Entropy              | 0.13737    |
| Perplexity           | 1.1472     |
| AveragePolicyProb[0] | 0.50604    |
| AveragePolicyProb[1] | 0.49396    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1136       |
| TotalNSamples        | 1.9205e+05 |
| ExplainedVariance    | 0.2063     |
-------------------------------------
[2019-11-06 07:55:34.541544 UTC] Saving snapshot
[2019-11-06 07:55:34.555915 UTC] Starting iteration 97
[2019-11-06 07:55:34.556669 UTC] Start collecting samples
[2019-11-06 07:55:35.063000 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:35.108228 UTC] Computing policy gradient
[2019-11-06 07:55:35.126061 UTC] Updating baseline
[2019-11-06 07:55:35.327601 UTC] Computing logging information
-------------------------------------
| Iteration            | 97         |
| SurrLoss             | -0.0042873 |
| Entropy              | 0.13288    |
| Perplexity           | 1.1421     |
| AveragePolicyProb[0] | 0.49865    |
| AveragePolicyProb[1] | 0.50135    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1149       |
| TotalNSamples        | 1.9465e+05 |
| ExplainedVariance    | -0.10868   |
-------------------------------------
[2019-11-06 07:55:35.383540 UTC] Saving snapshot
[2019-11-06 07:55:35.396605 UTC] Starting iteration 98
[2019-11-06 07:55:35.396834 UTC] Start collecting samples
[2019-11-06 07:55:35.793780 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:35.817991 UTC] Computing policy gradient
[2019-11-06 07:55:35.831835 UTC] Updating baseline
[2019-11-06 07:55:36.049882 UTC] Computing logging information
--------------------------------------
| Iteration            | 98          |
| SurrLoss             | -0.00075876 |
| Entropy              | 0.13356     |
| Perplexity           | 1.1429      |
| AveragePolicyProb[0] | 0.49975     |
| AveragePolicyProb[1] | 0.50025     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 1156        |
| TotalNSamples        | 1.9605e+05  |
| ExplainedVariance    | -0.20679    |
--------------------------------------
[2019-11-06 07:55:36.110031 UTC] Saving snapshot
[2019-11-06 07:55:36.121863 UTC] Starting iteration 99
[2019-11-06 07:55:36.122073 UTC] Start collecting samples
[2019-11-06 07:55:36.548090 UTC] Computing input variables for policy optimization
[2019-11-06 07:55:36.577150 UTC] Computing policy gradient
[2019-11-06 07:55:36.590965 UTC] Updating baseline
[2019-11-06 07:55:36.758224 UTC] Computing logging information
-------------------------------------
| Iteration            | 99         |
| SurrLoss             | -0.010159  |
| Entropy              | 0.13051    |
| Perplexity           | 1.1394     |
| AveragePolicyProb[0] | 0.49931    |
| AveragePolicyProb[1] | 0.50069    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1167       |
| TotalNSamples        | 1.9825e+05 |
| ExplainedVariance    | -0.031666  |
-------------------------------------
[2019-11-06 07:55:36.814162 UTC] Saving snapshot
