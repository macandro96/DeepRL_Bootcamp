[2019-11-25 17:50:18.898228 UTC] Starting env pool
[2019-11-25 17:50:18.949987 UTC] Starting iteration 0
[2019-11-25 17:50:18.950307 UTC] Start collecting samples
[2019-11-25 17:50:19.216776 UTC] Computing input variables for policy optimization
[2019-11-25 17:50:19.274309 UTC] Performing policy update
[2019-11-25 17:50:19.274816 UTC] Computing gradient in Euclidean space
[2019-11-25 17:50:20.455157 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:50:26.917561 UTC] Performing line search
[2019-11-25 17:50:27.945519 UTC] Updating baseline
[2019-11-25 17:50:28.166453 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| ExpectedImprovement  | 0.030797   |
| ActualImprovement    | 0.022022   |
| ImprovementRatio     | 0.71507    |
| MeanKL               | 0.0068129  |
| Entropy              | 0.6925     |
| Perplexity           | 1.9987     |
| AveragePolicyProb[0] | 0.50155    |
| AveragePolicyProb[1] | 0.49845    |
| AverageReturn        | 23.462     |
| MinReturn            | 9          |
| MaxReturn            | 81         |
| StdReturn            | 11.748     |
| AverageEpisodeLength | 23.462     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 81         |
| StdEpisodeLength     | 11.748     |
| TotalNEpisodes       | 78         |
| TotalNSamples        | 1830       |
| ExplainedVariance    | -0.0058665 |
-------------------------------------
[2019-11-25 17:50:28.545280 UTC] Saving snapshot
[2019-11-25 17:50:28.550403 UTC] Starting iteration 1
[2019-11-25 17:50:28.550600 UTC] Start collecting samples
[2019-11-25 17:50:28.813833 UTC] Computing input variables for policy optimization
[2019-11-25 17:50:28.880052 UTC] Performing policy update
[2019-11-25 17:50:28.880450 UTC] Computing gradient in Euclidean space
[2019-11-25 17:50:30.078722 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:50:36.661423 UTC] Performing line search
[2019-11-25 17:50:38.708796 UTC] Updating baseline
[2019-11-25 17:50:38.849769 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| ExpectedImprovement  | 0.031618  |
| ActualImprovement    | 0.027792  |
| ImprovementRatio     | 0.87901   |
| MeanKL               | 0.0068018 |
| Entropy              | 0.68362   |
| Perplexity           | 1.981     |
| AveragePolicyProb[0] | 0.49815   |
| AveragePolicyProb[1] | 0.50185   |
| AverageReturn        | 24.73     |
| MinReturn            | 10        |
| MaxReturn            | 68        |
| StdReturn            | 12.29     |
| AverageEpisodeLength | 24.73     |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 68        |
| StdEpisodeLength     | 12.29     |
| TotalNEpisodes       | 153       |
| TotalNSamples        | 3753      |
| ExplainedVariance    | 0.25776   |
------------------------------------
[2019-11-25 17:50:39.275403 UTC] Saving snapshot
[2019-11-25 17:50:39.280379 UTC] Starting iteration 2
[2019-11-25 17:50:39.280523 UTC] Start collecting samples
[2019-11-25 17:50:39.525270 UTC] Computing input variables for policy optimization
[2019-11-25 17:50:39.566784 UTC] Performing policy update
[2019-11-25 17:50:39.567240 UTC] Computing gradient in Euclidean space
[2019-11-25 17:50:40.695607 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:50:47.093314 UTC] Performing line search
[2019-11-25 17:50:48.125301 UTC] Updating baseline
[2019-11-25 17:50:48.255179 UTC] Computing logging information
------------------------------------
| Iteration            | 2         |
| ExpectedImprovement  | 0.038525  |
| ActualImprovement    | 0.028546  |
| ImprovementRatio     | 0.74098   |
| MeanKL               | 0.0091362 |
| Entropy              | 0.66903   |
| Perplexity           | 1.9523    |
| AveragePolicyProb[0] | 0.51278   |
| AveragePolicyProb[1] | 0.48722   |
| AverageReturn        | 33.82     |
| MinReturn            | 11        |
| MaxReturn            | 99        |
| StdReturn            | 18.767    |
| AverageEpisodeLength | 33.82     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 99        |
| StdEpisodeLength     | 18.767    |
| TotalNEpisodes       | 205       |
| TotalNSamples        | 5759      |
| ExplainedVariance    | 0.28436   |
------------------------------------
[2019-11-25 17:50:48.639158 UTC] Saving snapshot
[2019-11-25 17:50:48.644205 UTC] Starting iteration 3
[2019-11-25 17:50:48.644350 UTC] Start collecting samples
[2019-11-25 17:50:48.876350 UTC] Computing input variables for policy optimization
[2019-11-25 17:50:48.901431 UTC] Performing policy update
[2019-11-25 17:50:48.901885 UTC] Computing gradient in Euclidean space
[2019-11-25 17:50:50.032681 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:50:56.438791 UTC] Performing line search
[2019-11-25 17:50:57.474783 UTC] Updating baseline
[2019-11-25 17:50:57.588739 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| ExpectedImprovement  | 0.042562  |
| ActualImprovement    | 0.032059  |
| ImprovementRatio     | 0.75322   |
| MeanKL               | 0.0071554 |
| Entropy              | 0.64821   |
| Perplexity           | 1.9121    |
| AveragePolicyProb[0] | 0.53555   |
| AveragePolicyProb[1] | 0.46445   |
| AverageReturn        | 39.32     |
| MinReturn            | 11        |
| MaxReturn            | 108       |
| StdReturn            | 23.452    |
| AverageEpisodeLength | 39.32     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 108       |
| StdEpisodeLength     | 23.452    |
| TotalNEpisodes       | 231       |
| TotalNSamples        | 7084      |
| ExplainedVariance    | 0.26632   |
------------------------------------
[2019-11-25 17:50:57.974604 UTC] Saving snapshot
[2019-11-25 17:50:57.979607 UTC] Starting iteration 4
[2019-11-25 17:50:57.979760 UTC] Start collecting samples
[2019-11-25 17:50:58.205700 UTC] Computing input variables for policy optimization
[2019-11-25 17:50:58.237474 UTC] Performing policy update
[2019-11-25 17:50:58.237861 UTC] Computing gradient in Euclidean space
[2019-11-25 17:50:59.365204 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:51:05.852376 UTC] Performing line search
[2019-11-25 17:51:06.877974 UTC] Updating baseline
[2019-11-25 17:51:06.990916 UTC] Computing logging information
------------------------------------
| Iteration            | 4         |
| ExpectedImprovement  | 0.040151  |
| ActualImprovement    | 0.025012  |
| ImprovementRatio     | 0.62296   |
| MeanKL               | 0.0056086 |
| Entropy              | 0.62259   |
| Perplexity           | 1.8638    |
| AveragePolicyProb[0] | 0.5278    |
| AveragePolicyProb[1] | 0.4722    |
| AverageReturn        | 52.54     |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 36.977    |
| AverageEpisodeLength | 52.54     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 36.977    |
| TotalNEpisodes       | 256       |
| TotalNSamples        | 9069      |
| ExplainedVariance    | 0.32024   |
------------------------------------
[2019-11-25 17:51:07.409653 UTC] Saving snapshot
[2019-11-25 17:51:07.415018 UTC] Starting iteration 5
[2019-11-25 17:51:07.415170 UTC] Start collecting samples
[2019-11-25 17:51:07.637339 UTC] Computing input variables for policy optimization
[2019-11-25 17:51:07.667615 UTC] Performing policy update
[2019-11-25 17:51:07.668022 UTC] Computing gradient in Euclidean space
[2019-11-25 17:51:08.861793 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:51:15.365250 UTC] Performing line search
[2019-11-25 17:51:16.397175 UTC] Updating baseline
[2019-11-25 17:51:16.508655 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| ExpectedImprovement  | 0.040287  |
| ActualImprovement    | 0.015358  |
| ImprovementRatio     | 0.38122   |
| MeanKL               | 0.0048701 |
| Entropy              | 0.6009    |
| Perplexity           | 1.8238    |
| AveragePolicyProb[0] | 0.53886   |
| AveragePolicyProb[1] | 0.46114   |
| AverageReturn        | 65.3      |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 43.765    |
| AverageEpisodeLength | 65.3      |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 43.765    |
| TotalNEpisodes       | 273       |
| TotalNSamples        | 10844     |
| ExplainedVariance    | 0.52397   |
------------------------------------
[2019-11-25 17:51:16.890813 UTC] Saving snapshot
[2019-11-25 17:51:16.895835 UTC] Starting iteration 6
[2019-11-25 17:51:16.895978 UTC] Start collecting samples
[2019-11-25 17:51:17.106092 UTC] Computing input variables for policy optimization
[2019-11-25 17:51:17.127865 UTC] Performing policy update
[2019-11-25 17:51:17.128354 UTC] Computing gradient in Euclidean space
[2019-11-25 17:51:18.257306 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:51:24.638714 UTC] Performing line search
[2019-11-25 17:51:25.669094 UTC] Updating baseline
[2019-11-25 17:51:25.791823 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| ExpectedImprovement  | 0.032289  |
| ActualImprovement    | 0.021494  |
| ImprovementRatio     | 0.66567   |
| MeanKL               | 0.0074451 |
| Entropy              | 0.59835   |
| Perplexity           | 1.8191    |
| AveragePolicyProb[0] | 0.52012   |
| AveragePolicyProb[1] | 0.47988   |
| AverageReturn        | 81.08     |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 56.08     |
| AverageEpisodeLength | 81.08     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 56.08     |
| TotalNEpisodes       | 286       |
| TotalNSamples        | 13046     |
| ExplainedVariance    | 0.55327   |
------------------------------------
[2019-11-25 17:51:26.175202 UTC] Saving snapshot
[2019-11-25 17:51:26.180205 UTC] Starting iteration 7
[2019-11-25 17:51:26.180349 UTC] Start collecting samples
[2019-11-25 17:51:26.392137 UTC] Computing input variables for policy optimization
[2019-11-25 17:51:26.409678 UTC] Performing policy update
[2019-11-25 17:51:26.410130 UTC] Computing gradient in Euclidean space
[2019-11-25 17:51:27.548974 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:51:33.945249 UTC] Performing line search
[2019-11-25 17:51:34.985967 UTC] Updating baseline
[2019-11-25 17:51:35.095863 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| ExpectedImprovement  | 0.048138  |
| ActualImprovement    | 0.016151  |
| ImprovementRatio     | 0.33552   |
| MeanKL               | 0.0091287 |
| Entropy              | 0.5869    |
| Perplexity           | 1.7984    |
| AveragePolicyProb[0] | 0.51492   |
| AveragePolicyProb[1] | 0.48508   |
| AverageReturn        | 86.41     |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 58.341    |
| AverageEpisodeLength | 86.41     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 58.341    |
| TotalNEpisodes       | 291       |
| TotalNSamples        | 13802     |
| ExplainedVariance    | 0.61086   |
------------------------------------
[2019-11-25 17:51:35.480614 UTC] Saving snapshot
[2019-11-25 17:51:35.485670 UTC] Starting iteration 8
[2019-11-25 17:51:35.485818 UTC] Start collecting samples
[2019-11-25 17:51:35.705691 UTC] Computing input variables for policy optimization
[2019-11-25 17:51:35.730095 UTC] Performing policy update
[2019-11-25 17:51:35.730490 UTC] Computing gradient in Euclidean space
[2019-11-25 17:51:36.861296 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:51:43.417038 UTC] Performing line search
[2019-11-25 17:51:44.500871 UTC] Updating baseline
[2019-11-25 17:51:44.666699 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| ExpectedImprovement  | 0.032954  |
| ActualImprovement    | 0.013728  |
| ImprovementRatio     | 0.41656   |
| MeanKL               | 0.0089254 |
| Entropy              | 0.57546   |
| Perplexity           | 1.778     |
| AveragePolicyProb[0] | 0.53174   |
| AveragePolicyProb[1] | 0.46826   |
| AverageReturn        | 108.73    |
| MinReturn            | 12        |
| MaxReturn            | 200       |
| StdReturn            | 64.261    |
| AverageEpisodeLength | 108.73    |
| MinEpisodeLength     | 12        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 64.261    |
| TotalNEpisodes       | 306       |
| TotalNSamples        | 16643     |
| ExplainedVariance    | 0.5108    |
------------------------------------
[2019-11-25 17:51:45.098055 UTC] Saving snapshot
[2019-11-25 17:51:45.103479 UTC] Starting iteration 9
[2019-11-25 17:51:45.103648 UTC] Start collecting samples
[2019-11-25 17:51:45.366914 UTC] Computing input variables for policy optimization
[2019-11-25 17:51:45.391212 UTC] Performing policy update
[2019-11-25 17:51:45.391643 UTC] Computing gradient in Euclidean space
[2019-11-25 17:51:46.540241 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:51:52.922144 UTC] Performing line search
[2019-11-25 17:51:53.948852 UTC] Updating baseline
[2019-11-25 17:51:54.054751 UTC] Computing logging information
------------------------------------
| Iteration            | 9         |
| ExpectedImprovement  | 0.02405   |
| ActualImprovement    | 0.017855  |
| ImprovementRatio     | 0.7424    |
| MeanKL               | 0.0094824 |
| Entropy              | 0.57524   |
| Perplexity           | 1.7776    |
| AveragePolicyProb[0] | 0.49864   |
| AveragePolicyProb[1] | 0.50136   |
| AverageReturn        | 125.7     |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 61.759    |
| AverageEpisodeLength | 125.7     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 61.759    |
| TotalNEpisodes       | 320       |
| TotalNSamples        | 18968     |
| ExplainedVariance    | 0.54403   |
------------------------------------
[2019-11-25 17:51:54.435212 UTC] Saving snapshot
[2019-11-25 17:51:54.440259 UTC] Starting iteration 10
[2019-11-25 17:51:54.440415 UTC] Start collecting samples
[2019-11-25 17:51:54.636038 UTC] Computing input variables for policy optimization
[2019-11-25 17:51:54.655675 UTC] Performing policy update
[2019-11-25 17:51:54.656127 UTC] Computing gradient in Euclidean space
[2019-11-25 17:51:55.777384 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:52:02.365699 UTC] Performing line search
[2019-11-25 17:52:03.643010 UTC] Updating baseline
[2019-11-25 17:52:03.752851 UTC] Computing logging information
------------------------------------
| Iteration            | 10        |
| ExpectedImprovement  | 0.029205  |
| ActualImprovement    | 0.0070202 |
| ImprovementRatio     | 0.24038   |
| MeanKL               | 0.0049543 |
| Entropy              | 0.57718   |
| Perplexity           | 1.781     |
| AveragePolicyProb[0] | 0.51091   |
| AveragePolicyProb[1] | 0.48909   |
| AverageReturn        | 135.29    |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 59.134    |
| AverageEpisodeLength | 135.29    |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 59.134    |
| TotalNEpisodes       | 330       |
| TotalNSamples        | 20537     |
| ExplainedVariance    | 0.50223   |
------------------------------------
[2019-11-25 17:52:04.167071 UTC] Saving snapshot
[2019-11-25 17:52:04.172045 UTC] Starting iteration 11
[2019-11-25 17:52:04.172188 UTC] Start collecting samples
[2019-11-25 17:52:04.397594 UTC] Computing input variables for policy optimization
[2019-11-25 17:52:04.417594 UTC] Performing policy update
[2019-11-25 17:52:04.418008 UTC] Computing gradient in Euclidean space
[2019-11-25 17:52:05.565218 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:52:12.313693 UTC] Performing line search
[2019-11-25 17:52:13.353687 UTC] Updating baseline
[2019-11-25 17:52:13.472756 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| ExpectedImprovement  | 0.024512  |
| ActualImprovement    | 0.0063104 |
| ImprovementRatio     | 0.25744   |
| MeanKL               | 0.0057628 |
| Entropy              | 0.57147   |
| Perplexity           | 1.7709    |
| AveragePolicyProb[0] | 0.52631   |
| AveragePolicyProb[1] | 0.47369   |
| AverageReturn        | 145.82    |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 57.716    |
| AverageEpisodeLength | 145.82    |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 57.716    |
| TotalNEpisodes       | 341       |
| TotalNSamples        | 22437     |
| ExplainedVariance    | 0.31557   |
------------------------------------
[2019-11-25 17:52:13.857920 UTC] Saving snapshot
[2019-11-25 17:52:13.862861 UTC] Starting iteration 12
[2019-11-25 17:52:13.863010 UTC] Start collecting samples
[2019-11-25 17:52:14.072110 UTC] Computing input variables for policy optimization
[2019-11-25 17:52:14.091995 UTC] Performing policy update
[2019-11-25 17:52:14.092490 UTC] Computing gradient in Euclidean space
[2019-11-25 17:52:15.234467 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:52:21.813642 UTC] Performing line search
[2019-11-25 17:52:22.848359 UTC] Updating baseline
[2019-11-25 17:52:22.959720 UTC] Computing logging information
------------------------------------
| Iteration            | 12        |
| ExpectedImprovement  | 0.030021  |
| ActualImprovement    | 0.020521  |
| ImprovementRatio     | 0.68355   |
| MeanKL               | 0.0074706 |
| Entropy              | 0.56822   |
| Perplexity           | 1.7651    |
| AveragePolicyProb[0] | 0.50389   |
| AveragePolicyProb[1] | 0.49611   |
| AverageReturn        | 155.98    |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 49.756    |
| AverageEpisodeLength | 155.98    |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 49.756    |
| TotalNEpisodes       | 353       |
| TotalNSamples        | 24422     |
| ExplainedVariance    | 0.57472   |
------------------------------------
[2019-11-25 17:52:23.343107 UTC] Saving snapshot
[2019-11-25 17:52:23.348375 UTC] Starting iteration 13
[2019-11-25 17:52:23.348519 UTC] Start collecting samples
[2019-11-25 17:52:23.559826 UTC] Computing input variables for policy optimization
[2019-11-25 17:52:23.580268 UTC] Performing policy update
[2019-11-25 17:52:23.580715 UTC] Computing gradient in Euclidean space
[2019-11-25 17:52:24.716469 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:52:31.231663 UTC] Performing line search
[2019-11-25 17:52:32.308689 UTC] Updating baseline
[2019-11-25 17:52:32.436369 UTC] Computing logging information
------------------------------------
| Iteration            | 13        |
| ExpectedImprovement  | 0.016747  |
| ActualImprovement    | 0.0094781 |
| ImprovementRatio     | 0.56596   |
| MeanKL               | 0.0070899 |
| Entropy              | 0.56001   |
| Perplexity           | 1.7507    |
| AveragePolicyProb[0] | 0.51011   |
| AveragePolicyProb[1] | 0.48989   |
| AverageReturn        | 166.98    |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 41.932    |
| AverageEpisodeLength | 166.98    |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 41.932    |
| TotalNEpisodes       | 366       |
| TotalNSamples        | 26789     |
| ExplainedVariance    | 0.55077   |
------------------------------------
[2019-11-25 17:52:32.865390 UTC] Saving snapshot
[2019-11-25 17:52:32.870734 UTC] Starting iteration 14
[2019-11-25 17:52:32.870895 UTC] Start collecting samples
[2019-11-25 17:52:33.423163 UTC] Computing input variables for policy optimization
[2019-11-25 17:52:33.447345 UTC] Performing policy update
[2019-11-25 17:52:33.447806 UTC] Computing gradient in Euclidean space
[2019-11-25 17:52:34.654556 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:52:44.270680 UTC] Performing line search
[2019-11-25 17:52:45.650027 UTC] Updating baseline
[2019-11-25 17:52:45.823160 UTC] Computing logging information
------------------------------------
| Iteration            | 14        |
| ExpectedImprovement  | 0.027797  |
| ActualImprovement    | 0.0040616 |
| ImprovementRatio     | 0.14612   |
| MeanKL               | 0.0047407 |
| Entropy              | 0.56193   |
| Perplexity           | 1.7541    |
| AveragePolicyProb[0] | 0.50187   |
| AveragePolicyProb[1] | 0.49813   |
| AverageReturn        | 172.05    |
| MinReturn            | 56        |
| MaxReturn            | 200       |
| StdReturn            | 37.392    |
| AverageEpisodeLength | 172.05    |
| MinEpisodeLength     | 56        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 37.392    |
| TotalNEpisodes       | 376       |
| TotalNSamples        | 28620     |
| ExplainedVariance    | 0.61533   |
------------------------------------
[2019-11-25 17:52:46.321259 UTC] Saving snapshot
[2019-11-25 17:52:46.326858 UTC] Starting iteration 15
[2019-11-25 17:52:46.327059 UTC] Start collecting samples
[2019-11-25 17:52:46.738208 UTC] Computing input variables for policy optimization
[2019-11-25 17:52:46.766385 UTC] Performing policy update
[2019-11-25 17:52:46.766985 UTC] Computing gradient in Euclidean space
[2019-11-25 17:52:48.407301 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:52:59.277769 UTC] Performing line search
[2019-11-25 17:53:00.870260 UTC] Updating baseline
[2019-11-25 17:53:01.039751 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| ExpectedImprovement  | 0.022687  |
| ActualImprovement    | 0.013513  |
| ImprovementRatio     | 0.5956    |
| MeanKL               | 0.0057329 |
| Entropy              | 0.55679   |
| Perplexity           | 1.7451    |
| AveragePolicyProb[0] | 0.50272   |
| AveragePolicyProb[1] | 0.49728   |
| AverageReturn        | 175.04    |
| MinReturn            | 56        |
| MaxReturn            | 200       |
| StdReturn            | 35.95     |
| AverageEpisodeLength | 175.04    |
| MinEpisodeLength     | 56        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 35.95     |
| TotalNEpisodes       | 385       |
| TotalNSamples        | 30357     |
| ExplainedVariance    | 0.81198   |
------------------------------------
[2019-11-25 17:53:01.686928 UTC] Saving snapshot
[2019-11-25 17:53:01.696068 UTC] Starting iteration 16
[2019-11-25 17:53:01.696336 UTC] Start collecting samples
[2019-11-25 17:53:02.317463 UTC] Computing input variables for policy optimization
[2019-11-25 17:53:02.351880 UTC] Performing policy update
[2019-11-25 17:53:02.352477 UTC] Computing gradient in Euclidean space
[2019-11-25 17:53:04.211224 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:53:13.387972 UTC] Performing line search
[2019-11-25 17:53:14.787802 UTC] Updating baseline
[2019-11-25 17:53:14.967189 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| ExpectedImprovement  | 0.021356  |
| ActualImprovement    | 0.012219  |
| ImprovementRatio     | 0.57214   |
| MeanKL               | 0.0080575 |
| Entropy              | 0.54645   |
| Perplexity           | 1.7271    |
| AveragePolicyProb[0] | 0.50401   |
| AveragePolicyProb[1] | 0.49599   |
| AverageReturn        | 177.42    |
| MinReturn            | 69        |
| MaxReturn            | 200       |
| StdReturn            | 33.957    |
| AverageEpisodeLength | 177.42    |
| MinEpisodeLength     | 69        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 33.957    |
| TotalNEpisodes       | 396       |
| TotalNSamples        | 32513     |
| ExplainedVariance    | 0.70691   |
------------------------------------
[2019-11-25 17:53:15.552791 UTC] Saving snapshot
[2019-11-25 17:53:15.560340 UTC] Starting iteration 17
[2019-11-25 17:53:15.560533 UTC] Start collecting samples
[2019-11-25 17:53:15.880319 UTC] Computing input variables for policy optimization
[2019-11-25 17:53:15.912561 UTC] Performing policy update
[2019-11-25 17:53:15.913359 UTC] Computing gradient in Euclidean space
[2019-11-25 17:53:17.524026 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:53:27.257558 UTC] Performing line search
[2019-11-25 17:53:28.685692 UTC] Updating baseline
[2019-11-25 17:53:28.833392 UTC] Computing logging information
------------------------------------
| Iteration            | 17        |
| ExpectedImprovement  | 0.018952  |
| ActualImprovement    | 0.013304  |
| ImprovementRatio     | 0.70201   |
| MeanKL               | 0.0096248 |
| Entropy              | 0.55295   |
| Perplexity           | 1.7384    |
| AveragePolicyProb[0] | 0.50748   |
| AveragePolicyProb[1] | 0.49252   |
| AverageReturn        | 178.65    |
| MinReturn            | 69        |
| MaxReturn            | 200       |
| StdReturn            | 34.186    |
| AverageEpisodeLength | 178.65    |
| MinEpisodeLength     | 69        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 34.186    |
| TotalNEpisodes       | 405       |
| TotalNSamples        | 34308     |
| ExplainedVariance    | 0.78423   |
------------------------------------
[2019-11-25 17:53:29.364347 UTC] Saving snapshot
[2019-11-25 17:53:29.369917 UTC] Starting iteration 18
[2019-11-25 17:53:29.370101 UTC] Start collecting samples
[2019-11-25 17:53:29.720033 UTC] Computing input variables for policy optimization
[2019-11-25 17:53:29.757966 UTC] Performing policy update
[2019-11-25 17:53:29.758774 UTC] Computing gradient in Euclidean space
[2019-11-25 17:53:31.543947 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:53:40.942596 UTC] Performing line search
[2019-11-25 17:53:42.272166 UTC] Updating baseline
[2019-11-25 17:53:42.422560 UTC] Computing logging information
-----------------------------------
| Iteration            | 18       |
| ExpectedImprovement  | 0.020206 |
| ActualImprovement    | 0.013894 |
| ImprovementRatio     | 0.6876   |
| MeanKL               | 0.007439 |
| Entropy              | 0.52458  |
| Perplexity           | 1.6897   |
| AveragePolicyProb[0] | 0.4811   |
| AveragePolicyProb[1] | 0.5189   |
| AverageReturn        | 181.79   |
| MinReturn            | 69       |
| MaxReturn            | 200      |
| StdReturn            | 32.578   |
| AverageEpisodeLength | 181.79   |
| MinEpisodeLength     | 69       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 32.578   |
| TotalNEpisodes       | 415      |
| TotalNSamples        | 36221    |
| ExplainedVariance    | 0.53088  |
-----------------------------------
[2019-11-25 17:53:42.991789 UTC] Saving snapshot
[2019-11-25 17:53:42.999979 UTC] Starting iteration 19
[2019-11-25 17:53:43.000195 UTC] Start collecting samples
[2019-11-25 17:53:43.437119 UTC] Computing input variables for policy optimization
[2019-11-25 17:53:43.470534 UTC] Performing policy update
[2019-11-25 17:53:43.471212 UTC] Computing gradient in Euclidean space
[2019-11-25 17:53:45.092573 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:53:54.467161 UTC] Performing line search
[2019-11-25 17:53:55.869295 UTC] Updating baseline
[2019-11-25 17:53:56.040375 UTC] Computing logging information
------------------------------------
| Iteration            | 19        |
| ExpectedImprovement  | 0.01787   |
| ActualImprovement    | 0.0069317 |
| ImprovementRatio     | 0.38791   |
| MeanKL               | 0.0051355 |
| Entropy              | 0.52577   |
| Perplexity           | 1.6918    |
| AveragePolicyProb[0] | 0.48838   |
| AveragePolicyProb[1] | 0.51162   |
| AverageReturn        | 184.39    |
| MinReturn            | 69        |
| MaxReturn            | 200       |
| StdReturn            | 30.514    |
| AverageEpisodeLength | 184.39    |
| MinEpisodeLength     | 69        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 30.514    |
| TotalNEpisodes       | 427       |
| TotalNSamples        | 38565     |
| ExplainedVariance    | 0.54963   |
------------------------------------
[2019-11-25 17:53:56.627862 UTC] Saving snapshot
[2019-11-25 17:53:56.637385 UTC] Starting iteration 20
[2019-11-25 17:53:56.637627 UTC] Start collecting samples
[2019-11-25 17:53:57.017154 UTC] Computing input variables for policy optimization
[2019-11-25 17:53:57.051545 UTC] Performing policy update
[2019-11-25 17:53:57.052445 UTC] Computing gradient in Euclidean space
[2019-11-25 17:53:58.739064 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:54:08.087218 UTC] Performing line search
[2019-11-25 17:54:09.642151 UTC] Updating baseline
[2019-11-25 17:54:09.809331 UTC] Computing logging information
------------------------------------
| Iteration            | 20        |
| ExpectedImprovement  | 0.020249  |
| ActualImprovement    | 0.0073769 |
| ImprovementRatio     | 0.3643    |
| MeanKL               | 0.0089851 |
| Entropy              | 0.52812   |
| Perplexity           | 1.6957    |
| AveragePolicyProb[0] | 0.50487   |
| AveragePolicyProb[1] | 0.49513   |
| AverageReturn        | 186.28    |
| MinReturn            | 69        |
| MaxReturn            | 200       |
| StdReturn            | 28.29     |
| AverageEpisodeLength | 186.28    |
| MinEpisodeLength     | 69        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 28.29     |
| TotalNEpisodes       | 436       |
| TotalNSamples        | 40365     |
| ExplainedVariance    | 0.64303   |
------------------------------------
[2019-11-25 17:54:10.395645 UTC] Saving snapshot
[2019-11-25 17:54:10.402293 UTC] Starting iteration 21
[2019-11-25 17:54:10.402485 UTC] Start collecting samples
[2019-11-25 17:54:10.764338 UTC] Computing input variables for policy optimization
[2019-11-25 17:54:10.799065 UTC] Performing policy update
[2019-11-25 17:54:10.799635 UTC] Computing gradient in Euclidean space
[2019-11-25 17:54:12.591212 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:54:22.266799 UTC] Performing line search
[2019-11-25 17:54:25.359277 UTC] Updating baseline
[2019-11-25 17:54:25.546771 UTC] Computing logging information
------------------------------------
| Iteration            | 21        |
| ExpectedImprovement  | 0.011712  |
| ActualImprovement    | 0.0071426 |
| ImprovementRatio     | 0.60986   |
| MeanKL               | 0.0068688 |
| Entropy              | 0.51822   |
| Perplexity           | 1.679     |
| AveragePolicyProb[0] | 0.50266   |
| AveragePolicyProb[1] | 0.49734   |
| AverageReturn        | 190.63    |
| MinReturn            | 83        |
| MaxReturn            | 200       |
| StdReturn            | 21.614    |
| AverageEpisodeLength | 190.63    |
| MinEpisodeLength     | 83        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 21.614    |
| TotalNEpisodes       | 447       |
| TotalNSamples        | 42565     |
| ExplainedVariance    | 0.54662   |
------------------------------------
[2019-11-25 17:54:26.097999 UTC] Saving snapshot
[2019-11-25 17:54:26.104791 UTC] Starting iteration 22
[2019-11-25 17:54:26.104966 UTC] Start collecting samples
[2019-11-25 17:54:26.461971 UTC] Computing input variables for policy optimization
[2019-11-25 17:54:26.494159 UTC] Performing policy update
[2019-11-25 17:54:26.494907 UTC] Computing gradient in Euclidean space
[2019-11-25 17:54:27.991201 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:54:37.656478 UTC] Performing line search
[2019-11-25 17:54:39.140170 UTC] Updating baseline
[2019-11-25 17:54:39.360681 UTC] Computing logging information
------------------------------------
| Iteration            | 22        |
| ExpectedImprovement  | 0.013051  |
| ActualImprovement    | 0.009995  |
| ImprovementRatio     | 0.76585   |
| MeanKL               | 0.0092547 |
| Entropy              | 0.52829   |
| Perplexity           | 1.696     |
| AveragePolicyProb[0] | 0.50263   |
| AveragePolicyProb[1] | 0.49737   |
| AverageReturn        | 193.57    |
| MinReturn            | 83        |
| MaxReturn            | 200       |
| StdReturn            | 17.478    |
| AverageEpisodeLength | 193.57    |
| MinEpisodeLength     | 83        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 17.478    |
| TotalNEpisodes       | 456       |
| TotalNSamples        | 44365     |
| ExplainedVariance    | 0.45376   |
------------------------------------
[2019-11-25 17:54:39.977156 UTC] Saving snapshot
[2019-11-25 17:54:39.985522 UTC] Starting iteration 23
[2019-11-25 17:54:39.985758 UTC] Start collecting samples
[2019-11-25 17:54:40.377086 UTC] Computing input variables for policy optimization
[2019-11-25 17:54:40.413931 UTC] Performing policy update
[2019-11-25 17:54:40.415043 UTC] Computing gradient in Euclidean space
[2019-11-25 17:54:42.005482 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:54:51.450434 UTC] Performing line search
[2019-11-25 17:54:52.878011 UTC] Updating baseline
[2019-11-25 17:54:53.072402 UTC] Computing logging information
------------------------------------
| Iteration            | 23        |
| ExpectedImprovement  | 0.027689  |
| ActualImprovement    | 0.013512  |
| ImprovementRatio     | 0.48797   |
| MeanKL               | 0.0060245 |
| Entropy              | 0.54924   |
| Perplexity           | 1.7319    |
| AveragePolicyProb[0] | 0.47882   |
| AveragePolicyProb[1] | 0.52118   |
| AverageReturn        | 194.56    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 18.883    |
| AverageEpisodeLength | 194.56    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 18.883    |
| TotalNEpisodes       | 466       |
| TotalNSamples        | 46245     |
| ExplainedVariance    | 0.49255   |
------------------------------------
[2019-11-25 17:54:53.604151 UTC] Saving snapshot
[2019-11-25 17:54:53.611055 UTC] Starting iteration 24
[2019-11-25 17:54:53.611254 UTC] Start collecting samples
[2019-11-25 17:54:53.923032 UTC] Computing input variables for policy optimization
[2019-11-25 17:54:53.956371 UTC] Performing policy update
[2019-11-25 17:54:53.956852 UTC] Computing gradient in Euclidean space
[2019-11-25 17:54:55.664914 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:55:05.158474 UTC] Performing line search
[2019-11-25 17:55:06.621570 UTC] Updating baseline
[2019-11-25 17:55:06.785381 UTC] Computing logging information
------------------------------------
| Iteration            | 24        |
| ExpectedImprovement  | 0.018267  |
| ActualImprovement    | 0.012156  |
| ImprovementRatio     | 0.66545   |
| MeanKL               | 0.0065661 |
| Entropy              | 0.5368    |
| Perplexity           | 1.7105    |
| AveragePolicyProb[0] | 0.488     |
| AveragePolicyProb[1] | 0.512     |
| AverageReturn        | 196.25    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 15.037    |
| AverageEpisodeLength | 196.25    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 15.037    |
| TotalNEpisodes       | 477       |
| TotalNSamples        | 48445     |
| ExplainedVariance    | 0.25438   |
------------------------------------
[2019-11-25 17:55:07.339733 UTC] Saving snapshot
[2019-11-25 17:55:07.346514 UTC] Starting iteration 25
[2019-11-25 17:55:07.346712 UTC] Start collecting samples
[2019-11-25 17:55:07.671686 UTC] Computing input variables for policy optimization
[2019-11-25 17:55:07.704179 UTC] Performing policy update
[2019-11-25 17:55:07.704739 UTC] Computing gradient in Euclidean space
[2019-11-25 17:55:09.316824 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:55:18.545551 UTC] Performing line search
[2019-11-25 17:55:20.117809 UTC] Updating baseline
[2019-11-25 17:55:20.312118 UTC] Computing logging information
------------------------------------
| Iteration            | 25        |
| ExpectedImprovement  | 0.017104  |
| ActualImprovement    | 0.012906  |
| ImprovementRatio     | 0.75457   |
| MeanKL               | 0.0072478 |
| Entropy              | 0.52527   |
| Perplexity           | 1.6909    |
| AveragePolicyProb[0] | 0.48982   |
| AveragePolicyProb[1] | 0.51018   |
| AverageReturn        | 197.04    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 14.451    |
| AverageEpisodeLength | 197.04    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 14.451    |
| TotalNEpisodes       | 487       |
| TotalNSamples        | 50442     |
| ExplainedVariance    | 0.40098   |
------------------------------------
[2019-11-25 17:55:20.909122 UTC] Saving snapshot
[2019-11-25 17:55:20.918225 UTC] Starting iteration 26
[2019-11-25 17:55:20.918472 UTC] Start collecting samples
[2019-11-25 17:55:21.336844 UTC] Computing input variables for policy optimization
[2019-11-25 17:55:21.371848 UTC] Performing policy update
[2019-11-25 17:55:21.372391 UTC] Computing gradient in Euclidean space
[2019-11-25 17:55:23.081601 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:55:32.863734 UTC] Performing line search
[2019-11-25 17:55:34.374265 UTC] Updating baseline
[2019-11-25 17:55:34.540045 UTC] Computing logging information
------------------------------------
| Iteration            | 26        |
| ExpectedImprovement  | 0.020456  |
| ActualImprovement    | 0.01366   |
| ImprovementRatio     | 0.66777   |
| MeanKL               | 0.0065644 |
| Entropy              | 0.54372   |
| Perplexity           | 1.7224    |
| AveragePolicyProb[0] | 0.50611   |
| AveragePolicyProb[1] | 0.49389   |
| AverageReturn        | 197.29    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 14.282    |
| AverageEpisodeLength | 197.29    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 14.282    |
| TotalNEpisodes       | 495       |
| TotalNSamples        | 52042     |
| ExplainedVariance    | 0.79909   |
------------------------------------
[2019-11-25 17:55:35.048063 UTC] Saving snapshot
[2019-11-25 17:55:35.055316 UTC] Starting iteration 27
[2019-11-25 17:55:35.055508 UTC] Start collecting samples
[2019-11-25 17:55:35.472764 UTC] Computing input variables for policy optimization
[2019-11-25 17:55:35.510424 UTC] Performing policy update
[2019-11-25 17:55:35.511174 UTC] Computing gradient in Euclidean space
[2019-11-25 17:55:37.224451 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:55:46.671273 UTC] Performing line search
[2019-11-25 17:55:47.942696 UTC] Updating baseline
[2019-11-25 17:55:48.134943 UTC] Computing logging information
------------------------------------
| Iteration            | 27        |
| ExpectedImprovement  | 0.017537  |
| ActualImprovement    | 0.010257  |
| ImprovementRatio     | 0.58485   |
| MeanKL               | 0.0090909 |
| Entropy              | 0.52944   |
| Perplexity           | 1.698     |
| AveragePolicyProb[0] | 0.49945   |
| AveragePolicyProb[1] | 0.50055   |
| AverageReturn        | 197.34    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 14.283    |
| AverageEpisodeLength | 197.34    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 14.283    |
| TotalNEpisodes       | 507       |
| TotalNSamples        | 54442     |
| ExplainedVariance    | 0.45788   |
------------------------------------
[2019-11-25 17:55:48.756921 UTC] Saving snapshot
[2019-11-25 17:55:48.764067 UTC] Starting iteration 28
[2019-11-25 17:55:48.764254 UTC] Start collecting samples
[2019-11-25 17:55:49.121947 UTC] Computing input variables for policy optimization
[2019-11-25 17:55:49.163636 UTC] Performing policy update
[2019-11-25 17:55:49.164174 UTC] Computing gradient in Euclidean space
[2019-11-25 17:55:50.850072 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2019-11-25 17:56:00.553417 UTC] Performing line search
[2019-11-25 17:56:01.609588 UTC] Updating baseline
[2019-11-25 17:56:01.716309 UTC] Computing logging information
------------------------------------
| Iteration            | 28        |
| ExpectedImprovement  | 0.02235   |
| ActualImprovement    | 0.016105  |
| ImprovementRatio     | 0.72058   |
| MeanKL               | 0.0074602 |
| Entropy              | 0.51317   |
| Perplexity           | 1.6706    |
| AveragePolicyProb[0] | 0.501     |
| AveragePolicyProb[1] | 0.499     |
| AverageReturn        | 198.21    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 12.609    |
| AverageEpisodeLength | 198.21    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 12.609    |
| TotalNEpisodes       | 517       |
| TotalNSamples        | 56442     |
| ExplainedVariance    | 0.84096   |
------------------------------------
[2019-11-25 17:56:02.102013 UTC] Saving snapshot
[2019-11-25 17:56:02.107314 UTC] Starting iteration 29
[2019-11-25 17:56:02.107460 UTC] Start collecting samples
[2019-11-25 17:56:02.307839 UTC] Computing input variables for policy optimization
[2019-11-25 17:56:02.327027 UTC] Performing policy update
[2019-11-25 17:56:02.327473 UTC] Computing gradient in Euclidean space
[2019-11-25 17:56:03.459664 UTC] Computing approximate natural gradient using conjugate gradient algorithm
